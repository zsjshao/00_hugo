+++
author = "zsjshao"
title = "01_k8s简介及部署"
date = "2020-05-01"
tags = ["kubernetes"]
categories = ["kubernetes"]

+++
## 1、kubernetes概述

Kubernetes是由谷歌开源的容器集群管理系统，为容器化的应用提供了资源调度、部署运行、服务发现、扩容及缩容等一整套功能。

### 1.1、Kubernetes简史

kubernetes（来自希腊语，意为“舵手”或“飞行员”）由Joe Beda 、Brendan Burns 和Craig McLuckie创立，而后Google的其他几位工程师，包括Brian Grant和Tim Hockin等加盟共同研发，并由Google在2014年首次对外宣布。Kubernetes的开发和设计都深受Google内部系统Borg的影响，事实上，它的许多顶级贡献者之前也是Borg系统的开发者。

Borg是Google内部使用的大规模集群管理系统，久负盛名。它建构于容器技术之上，目的是实现资源管理的自动化，以及跨多个数据中心的资源利用率最大化。2015 年4月，Borg论文《Large-scale cluster management at Google with Borg 》伴随Kubernetes的高调宣传被Google首次公开，人们终于有缘得窥其全貌。

事实上，正是由于诞生于容器世家Google ，并站在Borg 这个巨人的肩膀之上，充分受益于Borg 过去十数年间积累的经验和教训，Kubernetes一面世就立即广受关注和青睐，并迅速称霸了容器编排技术领域。很多人将Kubernetes视为Borg系统的一个开源实现版本，在Google内部，Kubernetes的原始代号曾经是Seven of Nine，即星际迷航中友好的“Borg”角色，它标识中的舵轮有七个轮辐就是对该项目代号的致意，如图1所示。

Kubernetes v1.O于2015年7月2日发布，紧随其后，Google与Linux 基金会合作组建Cloud Native Computing Foundation（云原生计算基金会，简称为CNCF），并将Kubernetes作为种子技术予以提供。这之后，Kubernetes进入了版本快速迭代期，从此不断地融入新功能，如Federation、Network Policy API、RBAC、CRD和CSI等等，并增加了对Windows系统的支持。

### 1.2、Kubernetes特性

1）自动装箱

- 根据资源需求和其他约束自动放置容器，同时不会牺牲可用性，并通过调度机制混合关键型应用和非关键型应用的工作负载于同一节点以提升资源利用率并节省更多资源。

2）自我修复

- 重新启动失败的容器，在节点不可用时，替换和重新调度节点上的容器，对用户定义的健康检查不响应的容器会被中止，并且在容器准备好服务之前不会把其向客户端广播。

3）水平扩展

- 使用简单的命令或 UI，或者根据CPU等资源的使用情况自动调整应用程序副本数。

4）服务发现和负载均衡

- 不需要修改您的应用程序来使用不熟悉的服务发现机制，Kubernetes 为容器提供了自己的 IP 地址和一组容器的单个 DNS 名称，并可以在它们之间进行负载均衡

5）自动发布和回滚

- Kubernetes 逐渐部署对应用程序或其配置的更改，同时监视应用程序运行状况，以确保它不会同时终止所有实例。 如果出现问题，Kubernetes会为您恢复更改，利用日益增长的部署解决方案的生态系统。

6）密钥和配置管理

- 部署和更新密钥和应用程序配置，不会重新编译您的镜像，不会在堆栈配置中暴露密钥(secrets)。

7）存储编排

- 自动安装您所选择的存储系统，无论是本地存储，如公有云提供商 GCP 或 AWS, 还是网络存储系统 NFS,iSCSI,Gluster,Ceph,Cinder或Flocker。

8）批处理

- 除了服务之外，Kubernetes还可以管理您的批处理和CI工作负载，如果需要，替换出现故障的容器。

### 1.3、Kubernetes角色

**Master**

Master组件提供集群的管理控制中心。Master是集群的网关和中枢，负责诸如为用户和客户端暴露API ，跟踪其他服务器的健康状态、以最优方式调度工作负载， 以及编排其他组件之间的通信等任务，它是用户或客户端与集群之间的核心联络点，并负责Kubernetes系统的大多数集中式管控逻辑。单个Master节点即可完成其所有的功能，但出于冗余及负载均衡等目的，生产环境中通常需要协同部署多个此类主机。Master节点类似于蜂群中的蜂王。

**Node**

Node是Kubernetes集群的工作节点，负责接收来自Master的工作指令并根据指令相应地创建或销毁Pod对象，以及调整网络规则以合理地路由和转发流量等。理论上讲， Node可以是任何形式的计算设备，不过Master 会统一将其抽象为Node对象进行管理。Node类似于蜂群中的工蜂，生产环境中，它们通常数量众多。

### 1.4、Kubernetes术语

1）Pod

- Kubernetes并不直接运行容器，而是使用一个抽象的资源对象来封装一个或者多个容器，这个抽象即为Pod，它也是Kubernetes的最小调度单元。同一Pod中的容器共享网络名称空间和存储资源，这些容器可经由本地环回接口lo直接通信，但彼此之间又在Mount、User及PID等名称空间上保持了隔离。尽管Pod中可以包含多个容器，但是作为最小调度单元，它应该尽可能地保持“小”，即通常只应该包含一个主容器，以及必要的辅助型容器（sidecar）

2）Label

- 标签（Label）是将资源进行分类的标识符，资源标签其实就是一个键值型（key/values)数据。标签旨在指定对象（如Pod等）辨识性的属性，这些属性仅对用户存在特定的意义，对Kubernetes集群来说并不直接表达核心系统语义。标签可以在对象创建时附加其上，并能够在创建后的任意时间进行添加和修改。一个对象可以拥有多个标签，一个标签也可以附加于多个对象（通常是同一类对象）之上

3）Selector

- 标签选择器（Selector）全称为“Label Selector”，它是一种根据Label来过滤符合条件的资源对象的机制。例如，将附有标签“role:backend”的所有Pod对象挑选出来归为一组就是标签选择器的一种应用，如图5所示。用户通常使用标签对资源对象进行分类，而后使用标签选择器挑选出它们，例如将其创建为某Service的端点。

4）Controller

- 尽管Pod 是Kubernetes的最小调度单元，但用户通常并不会直接部署及管理Pod 对象，而是要借助于另一类抽象控制器（Controller）对其进行管理。用于工作负载的控制器是一种管理Pod生命周期的资源抽象，它们是Kubernetes上的一类对象，而非单个资源对象，包括ReplicationController、ReplicaSet、Deployment、StatefulSet、Job等。

5）Service

- Service是建立在一组Pod对象之上的资源抽象，它通过标签选择器选定一组Pod对象，并为这组Pod对象定义一个统一的固定访问入口（通常是一个IP地址），若Kubernetes集群存在DNS附件，它就会在Service创建时为其自动配置一个DNS名称以便客户端进行服务发现。到达Service IP的请求将被负载均衡至其后的端点一一各个Pod对象之上，因此Service从本质上来讲是一个四层代理服务。另外，Service还可以将集群外部流量引入到集群中来。

6）Volume

- 存储卷（Volume）是独立于容器文件系统之外的存储空间，常用于扩展容器的存储空间并为它提供持久存储能力。Kubernetes集群上的存储卷大体可分为临时卷、本地卷和网络卷。临时卷和本地卷都位于Node本地，一旦Pod 被调度至其他Node，此种类型的存储卷将无法访问到，因此临时卷和本地卷通常用于数据缓存，持久化的数据则需要放置于持久卷(persistent volume）之上。

7）Name和Namespace

- 名称（Name）是Kubernetes集群中资源对象的标识符，它们的作用域通常是名称空间(Namespace），因此名称空间是名称的额外的限定机制。在同一个名称空间中，同一类型资源、对象的名称必须具有唯一性。名称空间通常用于实现租户或项目的资源隔离，从而形成逻辑分组，如图6所示。创建的Pod和Service等资源对象都属于名称空间级别，未指定时，它们都属于默认的名称空间“default”。

8）Annotation

- Annotation（注解）是另一种附加在对象之上的键值类型的数据，但它拥有更大的数据容量。Annotation常用于将各种非标识型元数据（metadata）附加到对象上，但它不能用于标识和选择对象，通常也不会被Kubernetes直接使用，其主要目的是方便工具或用户的阅读及查找等。

9）Ingress

- Kubernetes将Pod对象和外部网络环境进行了隔离，Pod和Service等对象间的通信都使用其内部专用地址进行，如若需要开放某些Pod对象提供给外部用户访问，则需要为其请求流量打开一个通往Kubernetes集群内部的通道，除Service之外， Ingress也是这类通道的实现方式之一。

### 1.5、kubernetes设计架构

```
https://www.kubernetes.org.cn/kubernetes设计架构
```

![install_01](http://images.zsjshao.cn/images/kubernetes/install_01.png)

### 1.6、k8s核心组件介绍：

`kube-apiserver`：Kubernetes API server为api对象验证并配置数据，包括 pods、 services、replicationcontrollers和其他api对象，API Server提供REST操作到集群共享状态的前端，所有其他组件通过它进行交互。

https://k8smeetup.github.io/docs/admin/kube-apiserver/

`Kubernetes scheduler`：Kubernetes scheduler是一个拥有丰富策略、能够感知拓扑变化、支持特定负载的功能组件，它对集群的可用性、性能表现以及容量都影响巨大。scheduler需要考虑独立的和集体的资源需求、服务质量需求、硬件/软件/策略限制、亲和与反亲和规范、数据位置、内部负载接口、截止时间等等。如有必要，特定的负载需求可以通过API暴露出来。

https://k8smeetup.github.io/docs/admin/kube-scheduler/

`kube-controller-manager`：Controller Manager 负责管理 Cluster 各种资源，保证资源处于预期的状态。Controller Manager 由多种 controller 组成，包括 replication controller、endpoints controller、namespace controller、serviceaccounts controller 等。

不同的 controller 管理不同的资源。例如 replication controller 管理 Deployment、StatefulSet、DaemonSet 的生命周期，namespace controller 管理 Namespace 资源。

当某个Node意外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。

https://k8smeetup.github.io/docs/admin/kube-controller-manager/

`kube-proxy`：Kubernetes 网络代理运行在 node 上。它反映了 node 上 Kubernetes API 中定义的服务，并可以通过一组后端进行简单的 TCP、UDP 流转发或循环模式（round robin)）的 TCP、UDP 转发。目前，服务的集群 IP 和端口是通过 Docker-links 兼容的环境变量发现的，这些环境变量指定了服务代码打开的端口。有一个可选的 addon 为这些集群 IP 提供集群 DNS。用户必须使用 apiserver API 创建一个服务来配置代理。

`kubelet`：kubelet 是运行在每个节点上的主要的“节点代理”，它按照 PodSpec 中的描述工作。 PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象。kubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。kubelet 不管理不是由 Kubernetes 创建的容器。

除了来自 apiserver 的 PodSpec ，还有 3 种方式可以将容器清单提供给 kubelet 。

- 文件：在命令行指定的一个路径，在这个路径下的文件将被周期性的监视更新，默认监视周期是 20 秒并可以通过参数配置。

- HTTP端点：在命令行指定的一个HTTP端点，该端点每 20 秒被检查一次并且可以通过参数配置检查周期。

- HTTP服务：kubelet 还可以监听 HTTP 服务并响应一个简单的 API 来创建一个新的清单。

kubelet具体功能如下：

- 想master汇报node节点的状态信息
- 接受指令并在Pod中创建docker容器
- 准备Pod所需的数据卷
- 返回Pod的运行状态
- 在node节点执行容器健康检查

`etcd`：etcd是Kubernetes提供默认的存储系统，保存所有集群数据，使用时需要为etcd数据提供备份计划，etcd天生支持分布式。因为有watch(观察者)的支持，各部件协调中的改变可以很快被察觉。

`Container runtime`：Container runtime负责镜像管理以及Pod和容器的真正运行（CRI）；

### 1.7、Add-ons

`CoreDNS`：在Kubernetes集群中调度运行提供DNS服务的Pod，同一集群中的其他Pod可使用此DNS服务解决主机名。Kubernetes自1.11版本开始默认使用CoreDNS项目为集群提供服务注册和服务发现的动态名称解析服务，之前的版本中用到的是kube-dns项目，而SkyDNS 则是更早一代的项目。

`DashBoard`： Kubernetes集群的全部功能都要基于Web的UI，来管理集群中的应用甚至是集群自身。

`Prometheus`：容器和节点的性能监控与分析系统，它收集并解析多种指标数据，如资源利用率、生命周期事件等。Kubernetes自1.14版本开始默认使用Prometheus，之前版本使用Heapster。

`Ingress Controller`：Service是一种工作于传统层的负载均衡器，而Ingress 是在应用层实现的HTTP (s）负载均衡机制。不过，Ingress资源自身并不能进行“流量穿透”，它仅是一组路由规则的集合，这些规则需要通过Ingress控制器（ Ingress Controller)发挥作用。目前，此类的可用项目有Nginx、Traefik、Envoy及HAProxy等。

### 1.8、Kubernetes集群网络

Kubernetes的网络中主要存在四种类型的通信：同－Pod内的容器间通信、各Pod彼此之间的通信、Pod与Service间的通信以及集群外部的流量同Service之间的通信。Kubernetes为Pod和Service资源对象分别使用了各自的专用网络，Pod网络由Kubernetes的网络插件配置实现，而Service的网络则由Kubernetes 集群予以指定。为了提供更灵活的解决方式，Kubernetes的网络模型需要借助于外部插件实现，它要求任何实现机制都必须满足以下需求。

- 1、所有Pod间均可不经NAT 机制而直接通信。

- 2、所有节点均可不经NAT机制而直接与所有容器通信。

- 3、容器自己使用IP也是其他容器或节点直接看到的地址。换句话讲，所有Pod对象都位于同一平面网络中，而且可以使用Pod自身的地址直接通信。

现今大多都采用flannel提供网络，calico提供网络策略控制。



CNCF 云原生容器生态系统概要：

```
http://dockone.io/article/3006
```



### 1.8、安装方式：

**部署工具**

使用批量部署工具如（ansible/ Saltstack）、`手动二进制`、apt-get/yum等方式安装，以守护进程的方式启动在宿主机上。

**kubeadm**

使用k8s官方提供的部署工具kubeadm自动安装，需要在master和node节点上安装docker等组件，然后初始化，把管理端的控制服务和node上的服务都以pod的方式运行

## 2、kubeadm方式部署(单master)

https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/

安装注意事项

禁用`swap`、`selinux`、`iptables`

### 2.1、安装步骤

1、master和node节点先安装kubelet、docker、kubeadm

2、master节点运行kubeadm init初始化命令

3、验证master

4、node节点使用kubeadm加入k8s master

5、验证node

6、启动容器测试访问

### 2.2、安装docker

```
# step 1: 安装必要的一些系统工具
sudo apt-get update
sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common

# step 2: 安装GPG证书
curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -

# Step 3: 写入软件源信息
sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"

# Step 4: 更新并安装Docker-CE
sudo apt-get -y update
sudo apt-get -y install docker-ce
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://jr0tl680.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
docker info
```

### 2.3、配置阿里云仓库地址

```
apt-get update && apt-get install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
cat > /etc/apt/sources.list.d/kubernetes.list << EOF 
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt-get update
```

### 2.4、安装指定版本的kubeadm

```
kubeadm version #查看当前kubeadm版本
apt-cache madison kubeadm  #查看版本信息
apt-get -y install kubeadm=1.17.3-00 kubelet=1.17.3-00 kubectl=1.17.3-00
```

### 2.5、镜像下载

可提前将镜像pull下来

```
root@k8s-controller:~# kubeadm config images list --kubernetes-version v1.17.3
k8s.gcr.io/kube-apiserver:v1.17.3
k8s.gcr.io/kube-controller-manager:v1.17.3
k8s.gcr.io/kube-scheduler:v1.17.3
k8s.gcr.io/kube-proxy:v1.17.3
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.4.3-0
k8s.gcr.io/coredns:1.6.5
```

初始化master

kubeadm命令使用：https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/

kubeadm初始化选项：https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/

```
--image-repository string     默认值："k8s.gcr.io"
选择用于拉取控制平面镜像的容器仓库

--pod-network-cidr string
指明 pod 网络可以使用的 IP 地址段。如果设置了这个参数，控制平面将会为每一个节点自动分配 CIDRs。

--service-cidr string     默认值："10.96.0.0/12"
为服务的虚拟 IP 地址另外指定 IP 地址段

--service-dns-domain string     默认值："cluster.local"
为服务另外指定域名，例如："myorg.internal"。
```

`初始化命令`:

```
kubeadm init --kubernetes-version=v1.17.3 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers
```

kubeadm init流程：https://k8smeetup.github.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#init-workflow

### 2.6、配置kubectl证书

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

### 2.7、部署flannel

```
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

### 2.8、添加node节点

命令由kubeadm初始化生成，`在node节点执行`

```
kubeadm join 192.168.3.151:6443 --token ievyhu.we2giy989bvkbrca \
--discovery-token-ca-cert-hash sha256:f5a570fed91a24ee0a991bcfe1dfcceefb331723070f1ce14078459452158eea
```

### 2.9、查看节点数量

```
root@k8s-controller:~# kubectl get nodes
NAME                         STATUS   ROLES    AGE    VERSION
k8s-controller.zsjshao.net   Ready    master   16m    v1.17.3
k8s-node1.zsjshao.net        Ready    <none>   10m    v1.17.3
k8s-node2.zsjshao.net        Ready    <none>   7m8s   v1.17.3
```

### 2.10、查看pods

```
root@k8s-controller:~# kubectl get pods -n kube-system
NAME                                                 READY   STATUS    RESTARTS   AGE
coredns-7f9c544f75-fcsvh                             1/1     Running   0          18m
coredns-7f9c544f75-wn9xw                             1/1     Running   0          18m
etcd-k8s-controller.zsjshao.net                      1/1     Running   0          17m
kube-apiserver-k8s-controller.zsjshao.net            1/1     Running   0          17m
kube-controller-manager-k8s-controller.zsjshao.net   1/1     Running   0          17m
kube-flannel-ds-amd64-9jmjg                          1/1     Running   0          4m28s
kube-flannel-ds-amd64-n7ldg                          1/1     Running   0          4m28s
kube-flannel-ds-amd64-r92xz                          1/1     Running   0          4m28s
kube-proxy-4cgbz                                     1/1     Running   0          12m
kube-proxy-6x4j9                                     1/1     Running   0          18m
kube-proxy-9gl4s                                     1/1     Running   0          9m10s
kube-scheduler-k8s-controller.zsjshao.net            1/1     Running   0          17m
```

### 2.11、kubeadm升级k8s集群

`查看版本` apt-cache madison kubeadm

#### 2.11.1、升级kubeadm

```
root@k8s-controller:~# apt install kubeadm=1.17.4-00  #指定要安装的版本
root@k8s-controller:~# kubeadm version #查看版本
```

查看升级计划

```
root@k8s-controller:~# kubeadm upgrade plan
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.17.3
[upgrade/versions] kubeadm version: v1.17.4
I0329 18:33:12.791912   34318 version.go:251] remote version is much newer: v1.18.0; falling back to: stable-1.17
[upgrade/versions] Latest stable version: v1.17.4
[upgrade/versions] Latest version in the v1.17 series: v1.17.4

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     3 x v1.17.3   v1.17.4

Upgrade to the latest version in the v1.17 series:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.17.3   v1.17.4
Controller Manager   v1.17.3   v1.17.4
Scheduler            v1.17.3   v1.17.4
Kube Proxy           v1.17.3   v1.17.4
CoreDNS              1.6.5     1.6.5
Etcd                 3.4.3     3.4.3-0

You can now apply the upgrade by executing the following command:

	kubeadm upgrade apply v1.17.4

_____________________________________________________________________
```

#### 2.11.2、开始升级

```
root@k8s-controller:~# kubeadm upgrade apply v1.17.4
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/version] You have chosen to change the cluster version to "v1.17.4"
[upgrade/versions] Cluster version: v1.17.3
[upgrade/versions] kubeadm version: v1.17.4
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd]
[upgrade/prepull] Prepulling image for component etcd.
[upgrade/prepull] Prepulling image for component kube-apiserver.
[upgrade/prepull] Prepulling image for component kube-controller-manager.
[upgrade/prepull] Prepulling image for component kube-scheduler.
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[upgrade/prepull] Prepulled image for component etcd.
[upgrade/prepull] Prepulled image for component kube-apiserver.
[upgrade/prepull] Prepulled image for component kube-scheduler.
[upgrade/prepull] Prepulled image for component kube-controller-manager.
[upgrade/prepull] Successfully prepulled the images for all the control plane components
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.17.4"...
Static pod: kube-apiserver-k8s-controller.zsjshao.net hash: c66e654b37f23f481a44769881d9a22d
Static pod: kube-controller-manager-k8s-controller.zsjshao.net hash: fbb4ea9e0ee96f3a03d6ac44d87c8ac4
Static pod: kube-scheduler-k8s-controller.zsjshao.net hash: 703c43ab97818f969f780a2cbf4d24b7
[upgrade/etcd] Upgrading to TLS for etcd
[upgrade/etcd] Non fatal issue encountered during upgrade: the desired etcd version for this Kubernetes version "v1.
17.4" is "3.4.3-0", but the current etcd version is "3.4.3". Won't downgrade etcd, instead just continue[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests044899585"
W0329 18:35:16.518863   35092 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using 
"Node,RBAC"[upgrade/staticpods] Preparing for "kube-apiserver" upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifes
t to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-03-29-18-35-14/kube-apiserver.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-apiserver-k8s-controller.zsjshao.net hash: c66e654b37f23f481a44769881d9a22d
Static pod: kube-apiserver-k8s-controller.zsjshao.net hash: c66e654b37f23f481a44769881d9a22d
Static pod: kube-apiserver-k8s-controller.zsjshao.net hash: 4d7c3abf49e78dcff116a2d5a505cfa9
[apiclient] Found 1 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component "kube-apiserver" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-controller-manager" upgrade
[upgrade/staticpods] Renewing controller-manager.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up ol
d manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-03-29-18-35-14/kube-controller-manager.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-controller-manager-k8s-controller.zsjshao.net hash: fbb4ea9e0ee96f3a03d6ac44d87c8ac4
Static pod: kube-controller-manager-k8s-controller.zsjshao.net hash: ad6eb4381a48126b80f27b0879b2cdec
[apiclient] Found 1 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-scheduler" upgrade
[upgrade/staticpods] Renewing scheduler.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifes
t to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-03-29-18-35-14/kube-scheduler.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-scheduler-k8s-controller.zsjshao.net hash: 703c43ab97818f969f780a2cbf4d24b7
Static pod: kube-scheduler-k8s-controller.zsjshao.net hash: 0621ae8690c69d1d72f746bc2de0667e
[apiclient] Found 1 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component "kube-scheduler" upgraded successfully!
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.17" in namespace kube-system with the configuration for the kubelet
s in the cluster[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-syste
m namespace[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long t
erm certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node B
ootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluste
r[addons]: Migrating CoreDNS Corefile
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.17.4". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven'
t already done so.
```

#### 2.11.3、升级kubelet

```
root@k8s-controller:~# kubeadm upgrade node config --kubelet-version 1.17.4
```

#### 2.11.4、各节点升级二进制包

```
apt-get -y install kubeadm=1.17.4-00 kubelet=1.17.4-00 kubectl=1.17.4-00
```

#### 2.11.5、查看版本

```
root@k8s-controller:~# kubectl get nodes
NAME                         STATUS   ROLES    AGE   VERSION
k8s-controller.zsjshao.net   Ready    master   67m   v1.17.4
k8s-node1.zsjshao.net        Ready    <none>   61m   v1.17.4
k8s-node2.zsjshao.net        Ready    <none>   58m   v1.17.4
```

## 3、二进制手动部署

### 3.1、负载均衡配置

keepalived

```
apt install keepalived haproxy -y
cat > /etc/keepalived/keepalived.conf <<EOF
global_defs {
   notification_email {
     root@zsjshao.com
   }
   notification_email_from keepalived@zsjshao.com
   smtp_server 127.0.0.1
   smtp_connect_timeout 30
   router_id k8s-haproxy01.zsjshao.net
   vrrp_skip_check_adv_addr
#   vrrp_strict
   vrrp_garp_interval 0
   vrrp_gna_interval 0
   vrrp_iptables
}
 
vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 80
    priority 100
    advert_int 1
    unicast_src_ip 192.168.3.191
    unicast_peer {
        192.168.3.192
    }
    authentication {
        auth_type PASS
        auth_pass 1122
    }
    virtual_ipaddress {
        192.168.3.200 dev eth0
    }
}
EOF
```

haproxy

```
listen k8s_api_nodes_6443
    bind 192.168.3.200:6443
    mode tcp
    log global
    server 192.168.3.181 192.168.3.181:6443  check inter 3000 fall 2 rise 5
    server 192.168.3.182 192.168.3.182:6443  check inter 3000 fall 2 rise 5
    server 192.168.3.183 192.168.3.183:6443  check inter 3000 fall 2 rise 5

listen k8s_api_etcd_2379
    bind 192.168.3.200:2379
    mode tcp
    log global
    server 192.168.3.186 192.168.3.186:2379  check inter 3000 fall 2 rise 5
    server 192.168.3.187 192.168.3.187:2379  check inter 3000 fall 2 rise 5
    server 192.168.3.188 192.168.3.188:2379  check inter 3000 fall 2 rise 5
```

### 3.2、域名解析

```
192.168.3.200  kubernetes-api.zsjshao.net
192.168.3.200  etcd.zsjshao.net
```

`使用haproxy进行反代`

### 3.3、下载etcd安装包

```
wget https://github.com/etcd-io/etcd/releases/download/v3.4.6/etcd-v3.4.6-linux-amd64.tar.gz
mkdir /tmp/bin/ -p
tar xf etcd-v3.4.6-linux-amd64.tar.gz
cp etcd-v3.4.6-linux-amd64/etcd /tmp/bin/
cp etcd-v3.4.6-linux-amd64/etcdctl /tmp/bin/
```

### 3.4、下载kubernetes二进制包

```
wget https://dl.k8s.io/v1.18.0/kubernetes-server-linux-amd64.tar.gz
tar xf kubernetes-server-linux-amd64.tar.gz
rm -rf kubernetes/server/bin/*.tar
rm -rf kubernetes/server/bin/*_tag
\cp kubernetes/server/bin/* /tmp/bin
```

### 3.5、下载CNI插件

```
wget https://github.com/containernetworking/plugins/releases/download/v0.8.5/cni-plugins-linux-amd64-v0.8.5.tgz
mkdir /tmp/cni/ -p
tar xf cni-plugins-linux-amd64-v0.8.5.tgz -C /tmp/cni/
```

### 3.6、下载安装脚本

```
git clone  https://github.com/zsjshao/k8s-shell-install.git
tree k8s-shell-install/
k8s-shell-install/
├── 00-env.sh
├── 01-prepare.sh
├── 02-etcd.sh
├── 03-kube-master.sh
├── 04-kube-node.sh
├── canal.yaml
├── coredns.yaml.sed
├── deploy.sh
├── kube-flannel.yml
└── README.md
```

`修改00-env.sh环境配置文件，依次执行01-prepare.sh、02-etcd.sh、03-kube-master.sh、04-kube-node.sh`

### 3.7、安装flannel

```
vim kube-flannel.yml
...
  net-conf.json: |
    {
      "Network": "172.20.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
kubectl apply -f kube-flannel.yml
```

### 3.8、安装calico

```
vim canal.yaml
...
  net-conf.json: |
    {
      "Network": "172.20.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
kubectl apply -f canal.yaml
```

### 3.9、安装coredns

```
bash deploy.sh -i 10.68.0.10 -r "10.68.0.0/16" -s -t coredns.yaml.sed | kubectl apply -f -
```

### 3.10、查看集群状态

```
root@k8s-master01:~# kubectl get nodes
NAME            STATUS   ROLES    AGE   VERSION
192.168.3.181   Ready    <none>   22m   v1.18.0
192.168.3.182   Ready    <none>   22m   v1.18.0
192.168.3.183   Ready    <none>   22m   v1.18.0
192.168.3.189   Ready    <none>   22m   v1.18.0
192.168.3.190   Ready    <none>   22m   v1.18.0
```

## 4、ansible部署

https://github.com/easzlab/kubeasz

## 5、安装dashboard





