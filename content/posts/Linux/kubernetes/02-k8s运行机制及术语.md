+++
author = "zsjshao"
title = "02_k8s运行机制及术语"
date = "2020-05-01"
tags = ["kubernetes"]
categories = ["kubernetes"]

+++

## 1、master运行机制：

### 1.1、kube-apiserver：

k8s API Server提供了k8s各类资源对象（pod,RC,Service等）的增删改查及watch等HTTP Rest接口，是整个系统
的数据总线和数据中心。

apiserver 目前在master监听两个端口，通过 --insecure-port int 监听一个非安全的127.0.0.1本地端口（默认为
8080）

```
该端口用于接收HTTP请求；
该端口默认值为8080，可以通过API Server的启动参数“--insecure-port”的值来修改默认值；
默认的IP地址为“localhost”，可以通过启动参数“--insecure-bind-address”的值来修改该IP地址；
非认证或授权的HTTP请求通过该端口访问API Server。
```

通过参数--bind-address=1.1.1.1 监听一个安全的端口（默认为6443）

```
该端口默认值为6443，可通过启动参数“--secure-port”的值来修改默认值；
默认IP地址为非本地（Non-Localhost）网络端口，通过启动参数“--bind-address”设置该值；
该端口用于接收HTTPS请求；
用于基于Tocken文件或客户端证书及HTTP Base的认证；
用于基于策略的授权；
```

kubernetes API Server的功能与使用：

```
提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)；
提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直
接操作etcd）;
是资源配额控制的入口；
拥有完备的集群安全机制.
# curl 127.0.0.1:8080/apis #分组api
# curl 127.0.0.1:8080/api/v1 #带具体版本号的api
# curl 127.0.0.1:8080/ #返回核心api列表
# curl 127.0.0.1:8080/version #api 版本信息
# curl 127.0.0.1:8080/healthz/etcd #与etcd的心跳监测
# curl 127.0.0.1:8080/apis/autoscaling/v1 #api的详细信息
```

启动脚本：

```
root@u1:~# cat /lib/systemd/system/kube-apiserver.service 
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/k8s/kube/bin/kube-apiserver \
  --advertise-address=192.168.9.181 \
  --allow-privileged=true \
  --enable-aggregator-routing=true \
  --anonymous-auth=false \
  --authorization-mode=Node,RBAC \
  --bind-address=192.168.9.181 \
  --client-ca-file=/usr/local/k8s/kube/pki/zsjshao.net-ca.crt \
  --etcd-cafile=/usr/local/k8s/kube/pki/zsjshao.net-ca.crt \
  --etcd-certfile=/usr/local/k8s/kube/pki/admin.zsjshao.net.crt \
  --etcd-keyfile=/usr/local/k8s/kube/pki/admin.zsjshao.net.key \
  --etcd-servers=https://192.168.9.181:2379,https://192.168.9.182:2379,https://192.168.9.183:2379 \
  --kubelet-certificate-authority=/usr/local/k8s/kube/pki/zsjshao.net-ca.crt \
  --kubelet-client-certificate=/usr/local/k8s/kube/pki/admin.zsjshao.net.crt \
  --kubelet-client-key=/usr/local/k8s/kube/pki/admin.zsjshao.net.key \
  --kubelet-https=true \
  --tls-cert-file=/usr/local/k8s/kube/pki/kubernetes-api.zsjshao.net.crt \
  --tls-private-key-file=/usr/local/k8s/kube/pki/kubernetes-api.zsjshao.net.key \
  --proxy-client-cert-file=/usr/local/k8s/kube/pki/client.zsjshao.net.crt \
  --proxy-client-key-file=/usr/local/k8s/kube/pki/client.zsjshao.net.key \
  --requestheader-allowed-names= \
  --requestheader-client-ca-file=/usr/local/k8s/kube/pki/zsjshao.net-ca.crt \
  --requestheader-extra-headers-prefix=X-Remote-Extra- \
  --requestheader-group-headers=X-Remote-Group \
  --requestheader-username-headers=X-Remote-User \
  --service-account-key-file=/usr/local/k8s/kube/pki/zsjshao.net-ca.key \
  --service-cluster-ip-range=10.68.0.0/16 \
  --service-node-port-range=20000-40000 \
  --endpoint-reconciler-type=lease \
  --logtostderr=true \
  --v=2
Restart=always
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

### 1.2、kube-controller-manager：

Controller Manager作为集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命
名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理，当某个Node意
外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。

启动脚本：

```
root@u1:~# cat /lib/systemd/system/kube-controller-manager.service 
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/k8s/kube/bin/kube-controller-manager \
  --address=127.0.0.1 \
  --allocate-node-cidrs=true \
  --cluster-cidr=172.20.0.0/16 \
  --cluster-name=zsjshao.net \
  --cluster-signing-cert-file=/usr/local/k8s/kube/pki/zsjshao.net-ca.crt \
  --cluster-signing-key-file=/usr/local/k8s/kube/pki/zsjshao.net-ca.key \
  --kubeconfig=/usr/local/k8s/kube/conf/kube-controller-manager.conf \
  --leader-elect=true \
  --node-cidr-mask-size=24 \
  --root-ca-file=/usr/local/k8s/kube/pki/zsjshao.net-ca.crt \
  --service-account-private-key-file=/usr/local/k8s/kube/pki/zsjshao.net-ca.key \
  --service-cluster-ip-range=10.68.0.0/16 \
  --use-service-account-credentials=true \
  --v=2
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

### 1.3、kube-cheduler：

Scheduler负责Pod调度，在整个系统中起"承上启下"作用，承上：负责接收Controller Manager创建的新的Pod，为其选择一个合适的Node；启下：Node上的kubelet接管Pod的生命周期。

启动脚本：

```
root@u1:~# cat /lib/systemd/system/kube-scheduler.service 
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/k8s/kube/bin/kube-scheduler \
  --address=127.0.0.1 \
  --kubeconfig=/usr/local/k8s/kube/conf/kube-scheduler.conf \
  --leader-elect=true \
  --v=2
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

通过调度算法为待调度Pod列表的每个Pod从可用Node列表中选择一个最适合的Node，并将信息写入etcd中
node节点上的kubelet通过API Server监听到kubernetes Scheduler产生的Pod绑定信息，然后获取对应的
Pod清单，下载Image，并启动容器。

优选策略

```

1.LeastRequestedPriority
优先从备选节点列表中选择资源消耗最小的节点（CPU+内存）。
2.CalculateNodeLabelPriority
优先选择含有指定Label的节点。
3.BalancedResourceAllocation
优先从备选节点列表中选择各项资源使用率最均衡的节点。
```

## 2、node节点运行机制：

### 2.1、kubelet：

在kubernetes集群中，每个Node节点都会启动kubelet进程，用来处理Master节点下发到本节点的任务，管理
Pod和其中的容器。kubelet会在API Server上注册节点信息，定期向Master汇报节点资源使用情况，并通过
cAdvisor监控容器和节点资源。可以把kubelet理解成Server/Agent架构中的agent，kubelet是Node上的pod管
家。

启动脚本：

```
root@u1:~# cat /lib/systemd/system/kubelet.service 
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
WorkingDirectory=/usr/local/k8s/kube/kubelet_data/
ExecStartPre=/bin/mount -o remount,rw '/sys/fs/cgroup'
ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service
ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service
ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/memory/system.slice/kubelet.service
ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/pids/system.slice/kubelet.service
ExecStart=/usr/local/k8s/kube/bin/kubelet \
  --config=/usr/local/k8s/kube/conf/config192.168.9.181.yaml \
  --cni-bin-dir=/usr/local/k8s/kube/bin/ \
  --cni-conf-dir=/etc/cni/net.d/ \
  --hostname-override=192.168.9.181 \
  --kubeconfig=/usr/local/k8s/kube/conf/kubelet192.168.9.181.conf \
  --network-plugin=cni \
  --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.2 \
  --root-dir=/usr/local/k8s/kube/kubelet_data/ \
  --v=2
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

### 2.2、kube-proxy：

kube-proxy 运行在每个节点上，监听 API Server 中服务对象的变化，再通过管理 IPtables 来实现网络的转发。

Kube-Proxy 不同的版本可支持三种工作模式：

```
UserSpace
  k8s v1.2 后就已经淘汰

IPtables
  目前默认方式

IPVS
  需要安装ipvsadm、ipset 工具包和加载 ip_vs 内核模块
```

启动脚本：

```
root@u1:~# cat /lib/systemd/system/kube-proxy.service 
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
# kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后，kube-proxy 会对访问 Service IP 的请求做 SNAT
WorkingDirectory=/usr/local/k8s/kube/kube-proxy_data/
ExecStart=/usr/local/k8s/kube/bin/kube-proxy \
  --bind-address=192.168.9.181 \
  --cluster-cidr=172.20.0.0/16 \
  --hostname-override=192.168.9.181 \
  --kubeconfig=/usr/local/k8s/kube/conf/kube-proxy.conf \
  --logtostderr=true \
  --proxy-mode=ipvs
Restart=always
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

**iptables**：

Kube-Proxy 监听 Kubernetes Master 增加和删除 Service 以及 Endpoint 的消息。对于每一个 Service，Kube
Proxy 创建相应的 IPtables 规则，并将发送到 Service Cluster IP 的流量转发到 Service 后端提供服务的 Pod 的相
应端口上。 注： 虽然可以通过 Service 的 Cluster IP 和服务端口访问到后端 Pod 提供的服务，但该 Cluster IP 是
Ping 不通的。 其原因是 Cluster IP 只是 IPtables 中的规则，并不对应到一个任何网络设备。 IPVS 模式的 Cluster
IP 是可以 Ping 通的。

![run_01](http://images.zsjshao.cn/images/kubernetes/run_01.png)

**IPVS**：

kubernetes从1.9开始测试支持ipvs(Graduate kube-proxy IPVS mode to beta)，https://github.com/kubernete
s/kubernetes/blob/master/CHANGELOG-1.9.md#ipvs，从1.11版本正式支持ipvs(IPVS-based in-cluster load
balancing is now GA)，https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md#ipvs。

IPVS 相对 IPtables 效率会更高一些，使用 IPVS 模式需要在运行 Kube-Proxy 的节点上安装 ipvsadm、ipset 工具
包和加载 ip_vs 内核模块，当 Kube-Proxy 以 IPVS 代理模式启动时，Kube-Proxy 将验证节点上是否安装了 IPVS
模块，如果未安装，则 Kube-Proxy 将回退到 IPtables 代理模式。

```
使用IPVS模式，Kube-Proxy会监视Kubernetes Service对象和Endpoints，调用宿主机内核Netlink接口以
相应地创建IPVS规则并定期与Kubernetes Service对象 Endpoints对象同步IPVS规则，以确保IPVS状态与期
望一致，访问服务时，流量将被重定向到其中一个后端 Pod,IPVS使用哈希表作为底层数据结构并在内核空间中工
作，这意味着IPVS可以更快地重定向流量，并且在同步代理规则时具有更好的性能，此外，IPVS 为负载均衡算法
提供了更多选项，例如：rr (轮询调度)、lc (最小连接数)、dh (目标哈希)、sh (源哈希)、sed (最短期望延
迟)、nq(不排队调度)等。
```

![run_02](http://images.zsjshao.cn/images/kubernetes/run_02.png)



## 3、etcd运行机制：

etcd是CoreOS团队于2013年6月发起的开源项目，它的目标是构建一个高可用的分布式键值(key-value)数据库。

etcd内部采用raft协议作为一致性算法，etcd基于Go语言实现。

github地址：https://github.com/etcd-io/etcd

官方网站：https://etcd.io/

```
Etcd具有下面这些属性：
完全复制：集群中的每个节点都可以使用完整的存档
高可用性：Etcd可用于避免硬件的单点故障或网络问题
一致性：每次读取都会返回跨多主机的最新写入
简单：包括一个定义良好、面向用户的API（gRPC）
安全：实现了带有可选的客户端证书身份验证的自动化TLS
快速：每秒10000次写入的基准速度
可靠：使用Raft算法实现了存储的合理分布Etcd的工作原理
```

### 3.1、启动脚本参数：

```
root@u1:~# cat /lib/systemd/system/etcd.service 
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/usr/local/k8s/etcd/data/
ExecStart=/usr/local/k8s/etcd/bin/etcd \
  --name=etcd01 \
  --data-dir=/usr/local/k8s/etcd/data/ \
  --advertise-client-urls=https://192.168.9.181:2379 \
  --listen-client-urls=https://192.168.9.181:2379,https://127.0.0.1:2379 \
  --initial-advertise-peer-urls=https://192.168.9.181:2380 \
  --listen-peer-urls=https://192.168.9.181:2380 \
  --cert-file=/usr/local/k8s/etcd/pki/etcd_server.zsjshao.net.crt \
  --key-file=/usr/local/k8s/etcd/pki/etcd_server.zsjshao.net.key \
  --trusted-ca-file=/usr/local/k8s/etcd/pki/zsjshao.net-ca.crt \
  --peer-cert-file=/usr/local/k8s/etcd/pki/etcd_peer.zsjshao.net.crt \
  --peer-key-file=/usr/local/k8s/etcd/pki/etcd_peer.zsjshao.net.key \
  --peer-trusted-ca-file=/usr/local/k8s/etcd/pki/zsjshao.net-ca.crt \
  --initial-cluster-token=etcd-cluster-0 \
  --initial-cluster=etcd01=https://192.168.9.181:2380,etcd02=https://192.168.9.182:2380,etcd03=https://192.168.9.183:2380 \
  --initial-cluster-state=new \
  --snapshot-count=50000 \
  --auto-compaction-retention=1 \
  --max-request-bytes=10485760 \
  --quota-backend-bytes=8589934592
Restart=always
RestartSec=15
LimitNOFILE=65536
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target
```

### 3.2、查看成员信息：

Etcd v2 和 v3 本质上是共享同一套 raft 协议代码的两个独立的应用，接口不一样，存储不一样，数据互相隔离。
也就是说如果从 Etcd v2 升级到 Etcd v3，原来v2 的数据还是只能用 v2 的接口访问，v3 的接口创建的数据也只能
访问通过 v3 的接口访问。

WARNING: Environment variable ETCDCTL_API is not set; defaults to etcdctl v2. #默认使用V2版本 Set

environment variable ETCDCTL_API=3 to use v3 API or ETCDCTL_API=2 to use v2 API. #设置API版本

```
root@u1:~# ETCDCTL_API=3 /usr/local/k8s/etcd/bin/etcdctl member --help
NAME:
	member - Membership related commands

USAGE:
	etcdctl member <subcommand> [flags]

API VERSION:
	3.4


COMMANDS:
	add	Adds a member into the cluster
	list	Lists all members in the cluster
	promote	Promotes a non-voting member in the cluster
	remove	Removes a member from the cluster
	update	Updates a member in the cluster

OPTIONS:
  -h, --help[=false]	help for member
  

root@u1:~# ETCDCTL_API=3 /usr/local/k8s/etcd/bin/etcdctl member list --endpoints=https://192.168.9.181:2379 --cacert=/usr/local/k8s/etcd/pki/zsjshao.net-ca.crt --cert=/usr/local/k8s/etcd/pki/etcd_server.zsjshao.net.crt --key=/usr/local/k8s/etcd/pki/etcd_server.zsjshao.net.key
17d8f326c9fc34b, started, etcd03, https://192.168.9.183:2380, https://192.168.9.183:2379, false
484a2b3dc1486f72, started, etcd02, https://192.168.9.182:2380, https://192.168.9.182:2379, false
8a7b88a5c774adc5, started, etcd01, https://192.168.9.181:2380, https://192.168.9.181:2379, false

```

### 3.3、验证当前etcd成员状态：

```
root@u1:~# ETCDCTL_API=3 /usr/local/k8s/etcd/bin/etcdctl endpoint health --endpoints=https://192.168.9.181:2379 --cacert=/usr/local/k8s/etcd/pki/zsjshao.net-ca.crt --cert=/usr/local/k8s/etcd/pki/etcd_server.zsjshao.net.crt --key=/usr/local/k8s/etcd/pki/etcd_server.zsjshao.net.key
https://192.168.9.181:2379 is healthy: successfully committed proposal: took = 27.998252ms
```

### 3.4、查看etcd数据信息：

```
root@u1:~# ETCDCTL_API=3 /usr/local/k8s/etcd/bin/etcdctl get / --prefix --keys-only --endpoints=https://192.168.9.181:2379 --cacert=/usr/local/k8s/etcd/pki/zsjshao.net-ca.crt --cert=/usr/local/k8s/etcd/pki/etcd_server.zsjshao.net.crt --key=/usr/local/k8s/etcd/pki/etcd_server.zsjshao.net.key

root@u1:~# ETCDCTL_API=3 /usr/local/k8s/etcd/bin/etcdctl get /registry/serviceaccounts/kube-system/coredns --endpoints=https://192.168.9.181:2379 --cacert=/usr/local/k8s/etcd/pki/zsjshao.net-ca.crt --cert=/usr/local/k8s/etcd/pki/etcd_server.zsjshao.net.crt --key=/usr/local/k8s/etcd/pki/etcd_server.zsjshao.net.key
```

### 3.5、etcd增删改查数据：

```
#添加数据
root@u1:~# ETCDCTL_API=3 etcdctl put /testkey "test for linux36"
OK

#查询数据
root@u1:~# ETCDCTL_API=3 etcdctl get /testkey
/testkey
test for linux36

#改动数据
root@u1:~# ETCDCTL_API=3 etcdctl put /testkey "test for linux36-
new" #直接覆盖就是更新数据
OK

#验证改动
root@u1:~# ETCDCTL_API=3 etcdctl get /testkey
/testkey
test for linux36-new

#删除数据
root@u1:~# ETCDCTL_API=3 etcdctl del /testkey
1
root@u1:~# ETCDCTL_API=3 etcdctl get /testkey
```

### 3.6、etcd数据watch机制：

基于不断监看数据，发生变化就主动触发通知客户端，Etcd v3 的watch机制支持watch某个固定的key，也支持
watch一个范围。

相比Etcd v2, Etcd v3的一些主要变化：

```
接口通过grpc提供rpc接口，放弃了v2的http接口。优势是长连接效率提升明显，缺点是使用不如以前方便，尤其对不方便维护长连接的场景。
废弃了原来的目录结构，变成了纯粹的kv，用户可以通过前缀匹配模式模拟目录。
内存中不再保存value，同样的内存可以支持存储更多的key。
watch机制更稳定，基本上可以通过watch机制实现数据的完全同步。
提供了批量操作以及事务机制，用户可以通过批量事务请求来实现Etcd v2的CAS机制（批量事务支持if条件判断）。
```

watch测试：

```
#在etcd node1上watch一个key：
root@u1:~# ETCDCTL_API=3 etcdctl watch /testkey

#在etcd node2修改数据，验证etcd node1是否能够发现数据变化
root@u2:~# ETCDCTL_API=3 etcdctl put /testkey "test for linux36-new"
OK
```

### 3.7、etcd数据备份与恢复机制：

WAL是write ahead log的缩写，顾名思义，也就是在执行真正的写操作之前先写一个日志。

wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要
先写入到WAL中。

etcd v2版本数据备份与恢复：

```
V2版本帮助信息：
root@u1:~# ETCDCTL_API=2 etcdctl backup --help
NAME:
   etcdctl backup - backup an etcd directory

USAGE:
   etcdctl backup [command options]  

OPTIONS:
   --data-dir value        Path to the etcd data dir  #源数据目录
   --wal-dir value         Path to the etcd wal dir
   --backup-dir value      Path to the backup dir  #备份目录
   --backup-wal-dir value  Path to the backup wal dir
   --with-v3               Backup v3 backend data
   
V2版本备份数据：
root@u1:~# ETCDCTL_API=2 etcdctl backup --data-dir /usr/local/k8s/etcd/data/ --backup-dir /tmp/test/

V2版本恢复数据：
#恢复帮助信息：
root@u1:~# ETCDCTL_API=2 etcd --help | grep force
  --force-new-cluster 'false'
    force to create a new one-member cluster.

etcd --data-dir=/var/lib/etcd/default.etcd --force-new-cluster &

root@k8s-etcd2:~# vim /etc/systemd/system/etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos
[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/usr/bin/etcd \
  --name=etcd02 \
.................................
  --data-dir=/opt/etcd_backup -force-new-cluster #强制设置为为新集群
Restart=on-failure
RestartSec=5
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
```

etcd v3版本数据备份与恢复：

```
V3版本备份数据：
root@u1:~# ETCDCTL_API=3 etcdctl snapshot save /tmp/snapshot.db

V3版本恢复数据：
root@u1:~# ETCDCTL_API=3 etcdctl snapshot restore /tmp/snapshot.db --datadir=/
tmp/etcd-testdir

#自动备份数据
root@u1:~# mkdir /data/etcd-backup-dir/ -p
root@u1:~# cat script.sh
#!/bin/bash
source /etc/profile
DATE=`date +%Y-%m-%d_%H-%M-%S`
ETCDCTL_API=3 /usr/bin/etcdctl snapshot save /data/etcd-backup-dir/etcdsnapshot-${
DATE}.db

#注意证书验证
```

## 4、网络通信机制

k8s中的网络主要涉及到pod的的各种访问需求，如同一pod的内部(单容器或者多容器)通信、pod A与pod B的通
信、从外部网络访问pod以及从pod访问外部网络。

k8s的网络基于第三方插件实现，但是定义了一些插件兼容规范，该规范有CoreOS和Google联合定制，叫做
CNI(Container Network Interface)。

目前常用的的CNI网络插件有calico和flannel：

### 4.1、calico：

Calico是一个纯三层的网络解决方案，为容器提供多node间的访问通信，calico将每一个node节点都当做为一个路由器(router)，各节点通过BGP(Border Gateway Protocol) 边界网关协议学习并在node节点生成路由规则，从而将不同node节点上的pod连接起来进行通信。

BGP是一个去中心化的协议，它通过自动学习和维护路由表实现网络的可用性，但是并不是所有的网络都支持
BGP，另外为了跨网络实现更大规模的网络管理，calico 还支持IP-in-IP的叠加模型，简称IPIP，IPIP可以实现跨不
同网段建立路由通信，但是会存在安全性问题，其在内核内置，可以通过Calico的配置文件设置是否启用IPIP，在
公司内部如果k8s的node节点没有跨越网段建议关闭IPIP。

![run_03](http://images.zsjshao.cn/images/kubernetes/run_03.png)





### 4.2、flannel:

由CoreOS开源的针对k8s的网络服务，其目的为解决k8s集群中各主机上的pod相互通信的问题，其借助于etcd维
护网络IP地址分配，并为每一个node服务器分配一个不同的IP地址段。

Flannel 网络模型 (后端)，Flannel目前有三种方式实现 UDP/VXLAN/host-gw。：

```
UDP：早期版本的Flannel使用UDP封装完成报文的跨越主机转发，其安全性及性能略有不足。

VXLAN：Linux 内核在在2012年底的v3.7.0之后加入了VXLAN协议支持，因此新版本的Flannel也有UDP转换为
VXLAN，VXLAN本质上是一种tunnel（隧道）协议，用来基于3层网络实现虚拟的2层网络，目前flannel 的网络
模型已经是基于VXLAN的叠加(覆盖)网络。

Host-gw：也就是Host GateWay，通过在node节点上创建到达各目标容器地址的路由表而完成报文的转发，因此
这种方式要求各node节点本身必须处于同一个局域网(二层网络)中，因此不适用于网络变动频繁或比较大型的网络
环境，但是其性能较好。
```

Flannel 组件的解释：

```
Cni0:网桥设备，每创建一个pod都会创建一对 veth pair，其中一端是pod中的eth0，另一端是Cni0网桥中的
端口（网卡），Pod中从网卡eth0发出的流量都会发送到Cni0网桥设备的端口（网卡）上，Cni0 设备获得的ip地
址是该节点分配到的网段的第一个地址。

Flannel.1: overlay网络的设备，用来进行vxlan报文的处理（封包和解包），不同node之间的pod数据流量都
从overlay设备以隧道的形式发送到对端。
```

Flannel的系统文件及目录：

```
root@k8s-node2:~# find / -name flannel
/run/flannel
/opt/cni/bin/flannel
/var/lib/cni/flannel
```

#### 4.2.1、flannel pod状态

```
root@u1:~# kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
coredns-85b4878f78-qdppn                   1/1     Running   0          108m
kube-flannel-ds-75s85                      1/1     Running   1          7d
kube-flannel-ds-lqwxf                      1/1     Running   1          7d
kube-flannel-ds-nbbxr                      1/1     Running   1          7d
kube-flannel-ds-wjchk                      1/1     Running   1          7d
```

#### 4.2.2、当前node主机IP地址范围：

```
root@u1:~# cat /run/flannel/subnet.env 
FLANNEL_NETWORK=172.20.0.0/16
FLANNEL_SUBNET=172.20.1.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true
```

#### 4.2.3、当前node主机cni信息：

```
root@u1:~# cat /var/lib/cni/flannel/6233db63bf76d03925a484ca123794361a0e3fb8478940a7b008cdab980278ec 
{"cniVersion":"0.3.1","hairpinMode":true,"ipMasq":false,"ipam":{"routes":[{"dst":"172.20.0.0/16"}],"subnet":"172.20.1.0/24","type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}
root@u1:~# 
```

#### 4.2.4、当前node主机路由：

```
root@u1:~# ip route
default via 192.168.9.2 dev eth0 proto static 
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 
172.20.0.0/24 via 172.20.0.0 dev flannel.1 onlink 
172.20.1.0/24 dev cni0 proto kernel scope link src 172.20.1.1 
172.20.2.0/24 via 172.20.2.0 dev flannel.1 onlink 
172.20.3.0/24 via 172.20.3.0 dev flannel.1 onlink 
192.168.9.0/24 dev eth0 proto kernel scope link src 192.168.9.181
```

#### 4.2.5、验证跨网络pod通信：

```
root@u1:~# kubectl exec -it centos-0 -- bash
[root@centos-0 /]# ping www.baidu.com
PING www.a.shifen.com (163.177.151.109) 56(84) bytes of data.
64 bytes from 163.177.151.109 (163.177.151.109): icmp_seq=1 ttl=127 time=7.79 ms
64 bytes from 163.177.151.109 (163.177.151.109): icmp_seq=2 ttl=127 time=7.45 ms
^C
--- www.a.shifen.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 7.459/7.625/7.792/0.188 ms
[root@centos-0 /]# 
```

#### 4.2.6、VxLAN Directrouting：

Directrouting 为在同一个二层网络中的node节点启用直接路由机制，类似于host-gw模式。

修改flannel支持Directrouting

需要让配置文件在node节点重新生效，

```
root@u1:~# vim k8s/kube-flannel.yml
  net-conf.json: |
    {
      "Network": "172.20.0.0/16",
      "Backend": {
        "Type": "vxlan"
        "Directrouting": true
      }
    }
```

#### 4.2.7、Flannel不同node上的pod的通信流程：

Flannel.1 是一个overlay网络的设备，用来进行 vxlan 报文的处理（封包和解包），不同node之间的pod数据流量都从overlay设备以隧道的形式发送到对端。

![run_04](http://images.zsjshao.cn/images/kubernetes/run_04.png)

```
->: pod中产生数据，根据pod的路由信息，将数据发送到Cni0
->: Cni0 根据节点的路由表，将数据发送到隧道设备flannel.1
->: Flannel.1查看数据包的目的ip，从flanneld获得对端隧道设备的必要信息，封装数据包。
->: Flannel.1将数据包发送到对端设备,对端节点的网卡接收到数据包
->: 对端节点发现数据包为overlay数据包，解开外层封装，并发送到到本机flannel.1设备。
->: Flannel.1设备查看数据包，根据路由表匹配，将数据发送给Cni0设备。
->: Cni0匹配路由表，发送数据给网桥上对应的端口(pod)。
```

![run_05](http://images.zsjshao.cn/images/kubernetes/run_05.png)

