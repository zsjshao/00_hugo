+++
author = "zsjshao"
title = "01_k8s部署"
date = "2020-05-01"
tags = ["kubernetes"]
categories = ["kubernetes"]

+++
## 一、k8s

kubernetes设计架构

```
https://www.kubernetes.org.cn/kubernetes设计架构
```

![install_01](http://images.zsjshao.net/kubernetes/01-install/install_01.png)

### k8s核心组件介绍：

`kube-apiserver`：Kubernetes API server为api对象验证并配置数据，包括 pods、 services、replicationcontrollers和其他api对象，API Server提供REST操作到集群共享状态的前端，所有其他组件通过它进行交互。

https://k8smeetup.github.io/docs/admin/kube-apiserver/

`Kubernetes scheduler`：Kubernetes scheduler是一个拥有丰富策略、能够感知拓扑变化、支持特定负载的功能组件，它对集群的可用性、性能表现以及容量都影响巨大。scheduler需要考虑独立的和集体的资源需求、服务质量需求、硬件/软件/策略限制、亲和与反亲和规范、数据位置、内部负载接口、截止时间等等。如有必要，特定的负载需求可以通过API暴露出来。

https://k8smeetup.github.io/docs/admin/kube-scheduler/

`kube-controller-manager`：Controller Manager 负责管理 Cluster 各种资源，保证资源处于预期的状态。Controller Manager 由多种 controller 组成，包括 replication controller、endpoints controller、namespace controller、serviceaccounts controller 等。

不同的 controller 管理不同的资源。例如 replication controller 管理 Deployment、StatefulSet、DaemonSet 的生命周期，namespace controller 管理 Namespace 资源。

当某个Node意外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。

https://k8smeetup.github.io/docs/admin/kube-controller-manager/

`kube-proxy`：Kubernetes 网络代理运行在 node 上。它反映了 node 上 Kubernetes API 中定义的服务，并可以通过一组后端进行简单的 TCP、UDP 流转发或循环模式（round robin)）的 TCP、UDP 转发。目前，服务的集群 IP 和端口是通过 Docker-links 兼容的环境变量发现的，这些环境变量指定了服务代码打开的端口。有一个可选的 addon 为这些集群 IP 提供集群 DNS。用户必须使用 apiserver API 创建一个服务来配置代理。

`kubelet`：kubelet 是运行在每个节点上的主要的“节点代理”，它按照 PodSpec 中的描述工作。 PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象。kubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。kubelet 不管理不是由 Kubernetes 创建的容器。

除了来自 apiserver 的 PodSpec ，还有 3 种方式可以将容器清单提供给 kubelet 。

- 文件：在命令行指定的一个路径，在这个路径下的文件将被周期性的监视更新，默认监视周期是 20 秒并可以通过参数配置。

- HTTP端点：在命令行指定的一个HTTP端点，该端点每 20 秒被检查一次并且可以通过参数配置检查周期。

- HTTP服务：kubelet 还可以监听 HTTP 服务并响应一个简单的 API 来创建一个新的清单。

kubelet具体功能如下：

- 想master汇报node节点的状态信息
- 接受指令并在Pod中创建docker容器
- 准备Pod所需的数据卷
- 返回Pod的运行状态
- 在node节点执行容器健康检查

`etcd`：etcd是Kubernetes提供默认的存储系统，保存所有集群数据，使用时需要为etcd数据提供备份计划，etcd天生支持分布式。因为有watch(观察者)的支持，各部件协调中的改变可以很快被察觉。

`Container runtime`：Container runtime负责镜像管理以及Pod和容器的真正运行（CRI）；

### Add-ons

`kube-dns`：kube-dns负责为整个集群提供DNS服务

`DashBoard`：提供GUI



CNCF 云原生容器生态系统概要：

```
http://dockone.io/article/3006
```



### 安装方式：

#### 部署工具

使用批量部署工具如（ansible/ Saltstack）、`手动二进制`、apt-get/yum等方式安装，以守护进程的方式启动在宿主机上。

#### kubeadm

使用k8s官方提供的部署工具kubeadm自动安装，需要在master和node节点上安装docker等组件，然后初始化，把管理端的控制服务和node上的服务都以pod的方式运行

### kubeadm方式部署(单master)

https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/

安装注意事项

禁用`swap`、`selinux`、`iptables`

#### 安装步骤

1、master和node节点先安装kubelet、docker、kubeadm

2、master节点运行kubeadm init初始化命令

3、验证master

4、node节点使用kubeadm加入k8s master

5、验证node

6、启动容器测试访问

#### 安装docker

```
# step 1: 安装必要的一些系统工具
sudo apt-get update
sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common
# step 2: 安装GPG证书
curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
# Step 3: 写入软件源信息
sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"
# Step 4: 更新并安装Docker-CE
sudo apt-get -y update
sudo apt-get -y install docker-ce
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://jr0tl680.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
docker info
```

#### 配置阿里云仓库地址

```
apt-get update && apt-get install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
cat > /etc/apt/sources.list.d/kubernetes.list << EOF 
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt-get update
```

#### 安装指定版本的kubeadm

```
kubeadm version #查看当前kubeadm版本
apt-cache madison kubeadm  #查看版本信息
apt-get -y install kubeadm=1.17.3-00 kubelet=1.17.3-00 kubectl=1.17.3-00
```

#### 镜像下载

可提前将镜像pull下来

```
root@k8s-controller:~# kubeadm config images list --kubernetes-version v1.17.3
k8s.gcr.io/kube-apiserver:v1.17.3
k8s.gcr.io/kube-controller-manager:v1.17.3
k8s.gcr.io/kube-scheduler:v1.17.3
k8s.gcr.io/kube-proxy:v1.17.3
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.4.3-0
k8s.gcr.io/coredns:1.6.5
```

初始化master

kubeadm命令使用：https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/

kubeadm初始化选项：https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/

```
--image-repository string     默认值："k8s.gcr.io"
选择用于拉取控制平面镜像的容器仓库

--pod-network-cidr string
指明 pod 网络可以使用的 IP 地址段。如果设置了这个参数，控制平面将会为每一个节点自动分配 CIDRs。

--service-cidr string     默认值："10.96.0.0/12"
为服务的虚拟 IP 地址另外指定 IP 地址段

--service-dns-domain string     默认值："cluster.local"
为服务另外指定域名，例如："myorg.internal"。
```

`初始化命令`:

```
kubeadm init --kubernetes-version=v1.17.3 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers
```

kubeadm init流程：https://k8smeetup.github.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#init-workflow

#### 配置kubectl证书

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

#### 部署flannel

```
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

#### 添加node节点

命令由kubeadm初始化生成，`在node节点执行`

```
kubeadm join 192.168.3.151:6443 --token ievyhu.we2giy989bvkbrca \
--discovery-token-ca-cert-hash sha256:f5a570fed91a24ee0a991bcfe1dfcceefb331723070f1ce14078459452158eea
```

#### 查看节点数量

```
root@k8s-controller:~# kubectl get nodes
NAME                         STATUS   ROLES    AGE    VERSION
k8s-controller.zsjshao.net   Ready    master   16m    v1.17.3
k8s-node1.zsjshao.net        Ready    <none>   10m    v1.17.3
k8s-node2.zsjshao.net        Ready    <none>   7m8s   v1.17.3
```

#### 查看pods

```
root@k8s-controller:~# kubectl get pods -n kube-system
NAME                                                 READY   STATUS    RESTARTS   AGE
coredns-7f9c544f75-fcsvh                             1/1     Running   0          18m
coredns-7f9c544f75-wn9xw                             1/1     Running   0          18m
etcd-k8s-controller.zsjshao.net                      1/1     Running   0          17m
kube-apiserver-k8s-controller.zsjshao.net            1/1     Running   0          17m
kube-controller-manager-k8s-controller.zsjshao.net   1/1     Running   0          17m
kube-flannel-ds-amd64-9jmjg                          1/1     Running   0          4m28s
kube-flannel-ds-amd64-n7ldg                          1/1     Running   0          4m28s
kube-flannel-ds-amd64-r92xz                          1/1     Running   0          4m28s
kube-proxy-4cgbz                                     1/1     Running   0          12m
kube-proxy-6x4j9                                     1/1     Running   0          18m
kube-proxy-9gl4s                                     1/1     Running   0          9m10s
kube-scheduler-k8s-controller.zsjshao.net            1/1     Running   0          17m
```

#### kubeadm升级k8s集群

`查看版本` apt-cache madison kubeadm

##### 升级kubeadm

```
root@k8s-controller:~# apt install kubeadm=1.17.4-00  #指定要安装的版本
root@k8s-controller:~# kubeadm version #查看版本
```

查看升级计划

```
root@k8s-controller:~# kubeadm upgrade plan
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.17.3
[upgrade/versions] kubeadm version: v1.17.4
I0329 18:33:12.791912   34318 version.go:251] remote version is much newer: v1.18.0; falling back to: stable-1.17
[upgrade/versions] Latest stable version: v1.17.4
[upgrade/versions] Latest version in the v1.17 series: v1.17.4

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     3 x v1.17.3   v1.17.4

Upgrade to the latest version in the v1.17 series:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.17.3   v1.17.4
Controller Manager   v1.17.3   v1.17.4
Scheduler            v1.17.3   v1.17.4
Kube Proxy           v1.17.3   v1.17.4
CoreDNS              1.6.5     1.6.5
Etcd                 3.4.3     3.4.3-0

You can now apply the upgrade by executing the following command:

	kubeadm upgrade apply v1.17.4

_____________________________________________________________________
```

##### 开始升级

```
root@k8s-controller:~# kubeadm upgrade apply v1.17.4
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/version] You have chosen to change the cluster version to "v1.17.4"
[upgrade/versions] Cluster version: v1.17.3
[upgrade/versions] kubeadm version: v1.17.4
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd]
[upgrade/prepull] Prepulling image for component etcd.
[upgrade/prepull] Prepulling image for component kube-apiserver.
[upgrade/prepull] Prepulling image for component kube-controller-manager.
[upgrade/prepull] Prepulling image for component kube-scheduler.
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[upgrade/prepull] Prepulled image for component etcd.
[upgrade/prepull] Prepulled image for component kube-apiserver.
[upgrade/prepull] Prepulled image for component kube-scheduler.
[upgrade/prepull] Prepulled image for component kube-controller-manager.
[upgrade/prepull] Successfully prepulled the images for all the control plane components
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.17.4"...
Static pod: kube-apiserver-k8s-controller.zsjshao.net hash: c66e654b37f23f481a44769881d9a22d
Static pod: kube-controller-manager-k8s-controller.zsjshao.net hash: fbb4ea9e0ee96f3a03d6ac44d87c8ac4
Static pod: kube-scheduler-k8s-controller.zsjshao.net hash: 703c43ab97818f969f780a2cbf4d24b7
[upgrade/etcd] Upgrading to TLS for etcd
[upgrade/etcd] Non fatal issue encountered during upgrade: the desired etcd version for this Kubernetes version "v1.
17.4" is "3.4.3-0", but the current etcd version is "3.4.3". Won't downgrade etcd, instead just continue[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests044899585"
W0329 18:35:16.518863   35092 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using 
"Node,RBAC"[upgrade/staticpods] Preparing for "kube-apiserver" upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifes
t to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-03-29-18-35-14/kube-apiserver.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-apiserver-k8s-controller.zsjshao.net hash: c66e654b37f23f481a44769881d9a22d
Static pod: kube-apiserver-k8s-controller.zsjshao.net hash: c66e654b37f23f481a44769881d9a22d
Static pod: kube-apiserver-k8s-controller.zsjshao.net hash: 4d7c3abf49e78dcff116a2d5a505cfa9
[apiclient] Found 1 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component "kube-apiserver" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-controller-manager" upgrade
[upgrade/staticpods] Renewing controller-manager.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up ol
d manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-03-29-18-35-14/kube-controller-manager.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-controller-manager-k8s-controller.zsjshao.net hash: fbb4ea9e0ee96f3a03d6ac44d87c8ac4
Static pod: kube-controller-manager-k8s-controller.zsjshao.net hash: ad6eb4381a48126b80f27b0879b2cdec
[apiclient] Found 1 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-scheduler" upgrade
[upgrade/staticpods] Renewing scheduler.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifes
t to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-03-29-18-35-14/kube-scheduler.yaml"[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-scheduler-k8s-controller.zsjshao.net hash: 703c43ab97818f969f780a2cbf4d24b7
Static pod: kube-scheduler-k8s-controller.zsjshao.net hash: 0621ae8690c69d1d72f746bc2de0667e
[apiclient] Found 1 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component "kube-scheduler" upgraded successfully!
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.17" in namespace kube-system with the configuration for the kubelet
s in the cluster[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-syste
m namespace[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long t
erm certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node B
ootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluste
r[addons]: Migrating CoreDNS Corefile
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.17.4". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven'
t already done so.
```

##### 升级kubelet

```
root@k8s-controller:~# kubeadm upgrade node config --kubelet-version 1.17.4
```

##### 各节点升级二进制包

```
apt-get -y install kubeadm=1.17.4-00 kubelet=1.17.4-00 kubectl=1.17.4-00
```

##### 查看版本

```
root@k8s-controller:~# kubectl get nodes
NAME                         STATUS   ROLES    AGE   VERSION
k8s-controller.zsjshao.net   Ready    master   67m   v1.17.4
k8s-node1.zsjshao.net        Ready    <none>   61m   v1.17.4
k8s-node2.zsjshao.net        Ready    <none>   58m   v1.17.4
```

### 二进制手动部署

#### 负载均衡配置

keepalived

```
apt install keepalived haproxy -y
cat > /etc/keepalived/keepalived.conf <<EOF
global_defs {
   notification_email {
     root@zsjshao.com
   }
   notification_email_from keepalived@zsjshao.com
   smtp_server 127.0.0.1
   smtp_connect_timeout 30
   router_id k8s-haproxy01.zsjshao.net
   vrrp_skip_check_adv_addr
#   vrrp_strict
   vrrp_garp_interval 0
   vrrp_gna_interval 0
   vrrp_iptables
}
 
vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 80
    priority 100
    advert_int 1
    unicast_src_ip 192.168.3.191
    unicast_peer {
        192.168.3.192
    }
    authentication {
        auth_type PASS
        auth_pass 1122
    }
    virtual_ipaddress {
        192.168.3.200 dev eth0
    }
}
EOF
```

haproxy

```
listen k8s_api_nodes_6443
    bind 192.168.3.200:6443
    mode tcp
    log global
    server 192.168.3.181 192.168.3.181:6443  check inter 3000 fall 2 rise 5
    server 192.168.3.182 192.168.3.182:6443  check inter 3000 fall 2 rise 5
    server 192.168.3.183 192.168.3.183:6443  check inter 3000 fall 2 rise 5

listen k8s_api_etcd_2379
    bind 192.168.3.200:2379
    mode tcp
    log global
    server 192.168.3.186 192.168.3.186:2379  check inter 3000 fall 2 rise 5
    server 192.168.3.187 192.168.3.187:2379  check inter 3000 fall 2 rise 5
    server 192.168.3.188 192.168.3.188:2379  check inter 3000 fall 2 rise 5
```

#### 域名解析

```
192.168.3.200  kubernetes-api.zsjshao.net
192.168.3.200  etcd.zsjshao.net
```

`使用haproxy进行反代`

#### 下载etcd安装包

```
wget https://github.com/etcd-io/etcd/releases/download/v3.4.6/etcd-v3.4.6-linux-amd64.tar.gz
mkdir /tmp/bin/ -p
tar xf etcd-v3.4.6-linux-amd64.tar.gz
cp etcd-v3.4.6-linux-amd64/etcd /tmp/bin/
cp etcd-v3.4.6-linux-amd64/etcdctl /tmp/bin/
```

#### 下载kubernetes二进制包

```
wget https://dl.k8s.io/v1.18.0/kubernetes-server-linux-amd64.tar.gz
tar xf kubernetes-server-linux-amd64.tar.gz
rm -rf kubernetes/server/bin/*.tar
rm -rf kubernetes/server/bin/*_tag
\cp kubernetes/server/bin/* /tmp/bin
```

#### 下载CNI插件

```
wget https://github.com/containernetworking/plugins/releases/download/v0.8.5/cni-plugins-linux-amd64-v0.8.5.tgz
mkdir /tmp/cni/ -p
tar xf cni-plugins-linux-amd64-v0.8.5.tgz -C /tmp/cni/
```

#### 下载安装脚本

```
wget http://www.zsjshao.net:9999/zsjshao/k8s-shell-install/archive/master.tar.gz
tar xf master.tar.gz
root@k8s-master01:~# tree .
.
├── 00-env.sh
├── 01-prepare.sh
├── 02-etcd.sh
├── 03-kube-master.sh
├── 04-kube-node.sh
```

`修改00-env.sh环境配置文件，依次执行01-prepare.sh、02-etcd.sh、03-kube-master.sh、04-kube-node.sh`

#### 安装flannel

```
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

#### 查看集群状态

```
root@k8s-master01:~# kubectl get nodes
NAME            STATUS   ROLES    AGE   VERSION
192.168.3.181   Ready    <none>   22m   v1.18.0
192.168.3.182   Ready    <none>   22m   v1.18.0
192.168.3.183   Ready    <none>   22m   v1.18.0
192.168.3.189   Ready    <none>   22m   v1.18.0
192.168.3.190   Ready    <none>   22m   v1.18.0
```

### ansible部署

https://github.com/easzlab/kubeasz

安装dashboard





