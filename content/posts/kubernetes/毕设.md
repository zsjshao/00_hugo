+++
author = "zsjshao"
title = "k8s毕设"
date = "2020-05-01"
tags = ["kubernetes"]
categories = ["kubernetes"]
+++
 
 **摘   要**

 Kubernetes 是用于自动部署，扩展和管理容器化应用程序的开源系统。Kubernetes 用于协调高度可用的计算机集群，这些计算机群集被连接作为单个单元工作。Kubernetes 的抽象性允许您将容器化的应用程序部署到集群，而不必专门将其绑定到单个计算机。为了利用这种新的部署模型，应用程序需要以将它们与各个主机分离的方式打包: 它们需要被容器化。容器化应用程序比过去的部署模型更灵活和可用，其中应用程序直接安装到特定机器上，作为深入集成到主机中的软件包。 Kubernetes 在一个集群上以更有效的方式自动分发和调度容器应用程序。 

Kubernetes 是一个生产级的开源平台，用于协调计算机集群内部和跨计算机集群的应用程序容器的分发(调度)和运行。

 **关键词：** 1、容器   2、自动部署   3、kubernetes



 

# 一、kubernetes概述.

Kubernetes是由谷歌开源的容器集群管理系统，为容器化的应用提供了资源调度、部署运行、服务发现、扩容及缩容等一整套功能。

## （一）Kubernetes简史

kubernetes（来自希腊语，意为“舵手”或“飞行员”）由Joe Beda 、Brendan Burns 和Craig McLuckie创立，而后Google的其他几位工程师，包括Brian Grant和Tim Hockin等加盟共同研发，并由Google在2014年首次对外宣布。Kubernetes的开发和设计都深受Google内部系统Borg的影响，事实上，它的许多顶级贡献者之前也是Borg系统的开发者。

​                 图1 Kubernetes logo                

![img](file:///C:/Users/zengs/AppData/Local/Temp/msohtmlclip1/01/clip_image004.jpg)Borg是Google内部使用的大规模集群管理系统，久负盛名。它建构于容器技术之上，目的是实现资源管理的自动化，以及跨多个数据中心的资源利用率最大化。2015 年4月，Borg论文《Large-scale cluster management at Google with Borg 》伴随Kubernetes的高调宣传被Google首次公开，人们终于有缘得窥其全貌。

事实上，正是由于诞生于容器世家Google ，并站在Borg 这个巨人的肩膀之上，充分受益于Borg 过去十数年间积累的经验和教训，Kubernetes一面世就立即广受关注和青睐，并迅速称霸了容器编排技术领域。很多人将Kubernetes视为Borg系统的一个开源实现版本，在Google内部，Kubernetes的原始代号曾经是Seven of Nine，即星际迷航中友好的“Borg”角色，它标识中的舵轮有七个轮辐就是对该项目代号的致意，如图1所示。

Kubernetes v1.O于2015年7月2日发布，紧随其后，Google与Linux 基金会合作组建Cloud Native Computing Foundation（云原生计算基金会，简称为CNCF），并将Kubernetes作为种子技术予以提供。这之后，Kubernetes进入了版本快速迭代期，从此不断地融入新功能，如Federation、Network Policy API、RBAC、CRD和CSI等等，并增加了对Windows系统的支持。

## （二）Kubernetes特性

1、自动装箱

根据资源需求和其他约束自动放置容器，同时不会牺牲可用性，并通过调度机制混合关键型应用和非关键型应用的工作负载于同一节点以提升资源利用率并节省更多资源。

2、自我修复

重新启动失败的容器，在节点不可用时，替换和重新调度节点上的容器，对用户定义的健康检查不响应的容器会被中止，并且在容器准备好服务之前不会把其向客户端广播。

3、水平扩展

使用简单的命令或 UI，或者根据CPU等资源的使用情况自动调整应用程序副本数。

4、服务发现和负载均衡

不需要修改您的应用程序来使用不熟悉的服务发现机制，Kubernetes 为容器提供了自己的 IP 地址和一组容器的单个 DNS 名称，并可以在它们之间进行负载均衡

5、自动发布和回滚

Kubernetes 逐渐部署对应用程序或其配置的更改，同时监视应用程序运行状况，以确保它不会同时终止所有实例。 如果出现问题，Kubernetes会为您恢复更改，利用日益增长的部署解决方案的生态系统。

6、密钥和配置管理

部署和更新密钥和应用程序配置，不会重新编译您的镜像，不会在堆栈配置中暴露密钥(secrets)。

7、存储编排

自动安装您所选择的存储系统，无论是本地存储，如公有云提供商 GCP 或 AWS, 还是网络存储系统 NFS,iSCSI,Gluster,Ceph,Cinder或Flocker。

8、批处理

除了服务之外，Kubernetes还可以管理您的批处理和CI工作负载，如果需要，替换出现故障的容器。

## （二）Kubernetes角色

1、Master

​                 图2 Kubernetes集群主机                

![img](file:///C:/Users/zengs/AppData/Local/Temp/msohtmlclip1/01/clip_image006.jpg)Master组件提供集群的管理控制中心。Master是集群的网关和中枢，负责诸如为用户和客户端暴露API ，跟踪其他服务器的健康状态、以最优方式调度工作负载， 以及编排其他组件之间的通信等任务，它是用户或客户端与集群之间的核心联络点，并负责Kubernetes系统的大多数集中式管控逻辑。单个Master节点即可完成其所有的功能，但出于冗余及负载均衡等目的，生产环境中通常需要协同部署多个此类主机。Master节点类似于蜂群中的蜂王。

2、Node

Node是Kubernetes集群的工作节点，负责接收来自Master的工作指令并根据指令相应地创建或销毁Pod对象，以及调整网络规则以合理地路由和转发流量等。理论上讲， Node可以是任何形式的计算设备，不过Master 会统一将其抽象为Node对象进行管理。Node类似于蜂群中的工蜂，生产环境中，它们通常数量众多。

## （二）Kubernetes术语

1、Pod

​                 图3 Kubernetes Pod                

![img](file:///C:/Users/zengs/AppData/Local/Temp/msohtmlclip1/01/clip_image008.jpg)Kubernetes并不直接运行容器，而是使用一个抽象的资源对象来封装一个或者多个容器，这个抽象即为Pod，它也是Kubernetes的最小调度单元。同一Pod中的容器共享网络名称空间和存储资源，这些容器可经由本地环回接口lo直接通信，但彼此之间又在Mount、User及PID等名称空间上保持了隔离。尽管Pod中可以包含多个容器，但是作为最小调度单元，它应该尽可能地保持“小”，即通常只应该包含一个主容器，以及必要的辅助型容器（sidecar），如图3所示。

2、Label

​                 图4 Kubernetes 标签                

![img](file:///C:/Users/zengs/AppData/Local/Temp/msohtmlclip1/01/clip_image010.jpg)标签（Label）是将资源进行分类的标识符，资源标签其实就是一个键值型（key/values)数据。标签旨在指定对象（如Pod等）辨识性的属性，这些属性仅对用户存在特定的意义，对Kubernetes集群来说并不直接表达核心系统语义。标签可以在对象创建时附加其上，并能够在创建后的任意时间进行添加和修改。一个对象可以拥有多个标签，一个标签也可以附加于多个对象（通常是同一类对象）之上，如图4所示。

3、Selector



​                 图5 标签选择器                

​                ![img](file:///C:/Users/zengs/AppData/Local/Temp/msohtmlclip1/01/clip_image012.jpg)    
标签选择器（Selector）全称为“Label Selector”，它是一种根据Label来过滤符合条件的资源对象的机制。例如，将附有标签“role:backend”的所有Pod对象挑选出来归为一组就是标签选择器的一种应用，如图5所示。用户通常使用标签对资源对象进行分类，而后使用标签选择器挑选出它们，例如将其创建为某Service的端点。

4、Controller

尽管Pod 是Kubernetes的最小调度单元，但用户通常并不会直接部署及管理Pod 对象，而是要借助于另一类抽象控制器（Controller）对其进行管理。用于工作负载的控制器是一种管理Pod生命周期的资源抽象，它们是Kubernetes上的一类对象，而非单个资源对象，包括ReplicationController、ReplicaSet、Deployment、StatefulSet、Job等。

5、Service

Service是建立在一组Pod对象之上的资源抽象，它通过标签选择器选定一组Pod对象，并为这组Pod对象定义一个统一的固定访问入口（通常是一个IP地址），若Kubernetes集群存在DNS附件，它就会在Service创建时为其自动配置一个DNS名称以便客户端进行服务发现。到达Service IP的请求将被负载均衡至其后的端点一一各个Pod对象之上，因此Service从本质上来讲是一个四层代理服务。另外，Service还可以将集群外部流量引入到集群中来。

6、Volume

存储卷（Volume）是独立于容器文件系统之外的存储空间，常用于扩展容器的存储空间并为它提供持久存储能力。Kubernetes集群上的存储卷大体可分为临时卷、本地卷和网络卷。临时卷和本地卷都位于Node本地，一旦Pod 被调度至其他Node，此种类型的存储卷将无法访问到，因此临时卷和本地卷通常用于数据缓存，持久化的数据则需要放置于持久卷(persistent volume）之上。

7、Name和Namespace



​                 图6 名称空间                

​                ![img](file:///C:/Users/zengs/AppData/Local/Temp/msohtmlclip1/01/clip_image014.jpg)    
名称（Name）是Kubernetes集群中资源对象的标识符，它们的作用域通常是名称空间(Namespace），因此名称空间是名称的额外的限定机制。在同一个名称空间中，同一类型资源、对象的名称必须具有唯一性。名称空间通常用于实现租户或项目的资源隔离，从而形成逻辑分组，如图6所示。创建的Pod和Service等资源对象都属于名称空间级别，未指定时，它们都属于默认的名称空间“default”。

8、Annotation

Annotation（注解）是另一种附加在对象之上的键值类型的数据，但它拥有更大的数据容量。Annotation常用于将各种非标识型元数据（metadata）附加到对象上，但它不能用于标识和选择对象，通常也不会被Kubernetes直接使用，其主要目的是方便工具或用户的阅读及查找等。

9、Ingress

Kubernetes将Pod对象和外部网络环境进行了隔离，Pod和Service等对象间的通信都使用其内部专用地址进行，如若需要开放某些Pod对象提供给外部用户访问，则需要为其请求流量打开一个通往Kubernetes集群内部的通道，除Service之外， Ingress也是这类通道的实现方式之一。

## （三）Kubernetes集群组件

一个典型的Kubernetes集群由多个工作节点（worker node）和一个集群控制平面(control plane，即Master），以及一个集群状态存储系统（etcd）组成。其中Master 节点负责整个集群的管理工作，为集群提供管理接口，并监控和编排集群中的各个工作节点。各节点负责以Pod的形式运行容器，因此，各节点需要事先配置好容器运行依赖到的所有服务和资源，如容器运行时环境等。Kubernetes的系统架构如下图所示。Master 节点主要由kube-apiserver、kube-controller-manager和kube-scheduler三个组件，以及一个用于集群状态存储的etcd存储服务组成，而每个Node节点则主要包含kubelet、kube-proxy及容器引擎（Docker是最为常用的实现）等组件。此外，完整的集群服务还依赖于一些附加组件，如CoreDNS等。

1、Master组件

Master组件提供集群的管理控制中心。Master主要包含以下几个组件。

（1）API Server

kube-apiserver负责输出RESTful风格的Kubernetes API，它是发往集群的所有REST操作命令的接人点，并负责接收、校验并响应所有的REST请求，结果状态被持久存储于etcd中。因此，kube-apiserver是整个集群的网关。

（2）集群状态存储（Cluster State Store）

Kubernetes集群的所有状态信息都需要持久存储于存储系统etcd中，不过， etcd 是由CoreOS基于Raft协议开发的分布式键值存储，可用于服务发现、共享配置以及一致性保障（如数据库主节点选择、分布式锁等）。因此，etcd是独立的服务组件，并不隶属于Kubernetes集群自身。生产环境中应该以etcd集群的方式运行以确保其服务可用性。

etcd不仅能够提供键值数据存储，而且还为其提供了监听（ watch ）机制，用于监听和推送变更。Kubernetes集群系统中，etcd中的键值发生变化时会通知到API Server，并由其通过watch API向客户端输出。基于watch机制，Kubernetes集群的各组件实现了高效协同。

（3）控制器管理器（Controller Manager）

Kubernetes 中， 集群级别的大多数功能都是由几个被称为控制器的进程执行实现的，这几个进程被集成于kube-controller-manager守护进程中。由控制器完成的功能主要包括生命周期功能和API业务逻辑，具体如下。

生命周期功能：包括Namespace创建和生命周期、Event垃圾回收、Pod终止相关的垃圾回收、级联垃圾回收及Node垃圾回收等。

API 业务逻辑：例如，由ReplicaSet执行的Pod扩展等。

（4）调度器（Scheduler）

Kubernetes是用于部署和管理大规模容器应用的平台，根据集群规模的不同，其托管运行的容器很可能会数以千计甚至更多。API Server确认Pod对象的创建请求之后，便需要由Scheduler根据集群内各节点的可用资源状态，以及要运行的容器的资源需求做出调度决策。另外，Kubernetes还支持用户自定义调度器。

2、Node组件

Node负责提供运行容器的各种依赖环境，并接受Master的管理。每个Node主要由以下几个组件构成。

（1）Node的核心代理程序kubelet

kubelet是运行于工作节点之上的守护进程，它从API Server接收关于Pod对象的配置信息并确保它们处于期望的状态（desired state） 。kubelet会在API Server 上注册当前工作节点，定期向Master汇报节点资源使用情况，并通过cAdvisor监控容器和节点的资源占用状况。

（2）容器运行时环境

每个Node都要提供一个容器运行时（Container Runtime）环境，它负责下载镜像并运行容器。kubelet并未固定链接至某容器运行时环境，而是以插件的方式载入配置的容器环境。这种方式清晰地定义了各组件的边界。目前，Kubernetes支持的容器运行环境至少包括Docker、RKT、cri-o 和Fraki等。

（3）kube-proxy

每个工作节点都需要运行一个kube-proxy守护进程，它能够按需为Service资源对象生成iptables或ipvs规则，从而捕获访问当前Service的ClusterIP的流量并将其转发至正确的后端Pod 对象。

3、核心附件

Kubernetes 集群还依赖于一组称为“附件”（add-ons）的组件以提供完整的功能，它们通常是由第三方提供的特定应用程序，且托管运行于Kubernetes集群之上。

下面列出的几个附件各自为集群从不同角度引用了所需的核心功能。

（1）CoreDNS ：在Kubernetes集群中调度运行提供DNS服务的Pod，同一集群中的其他Pod可使用此DNS服务解决主机名。Kubernetes自1.11版本开始默认使用CoreDNS项目为集群提供服务注册和服务发现的动态名称解析服务，之前的版本中用到的是kube-dns项目，而SkyDNS 则是更早一代的项目。

（2）Kubernetes Dashboard : Kubernetes集群的全部功能都要基于Web的UI，来管理集群中的应用甚至是集群自身。

（3）Prometheus：容器和节点的性能监控与分析系统，它收集并解析多种指标数据，如资源利用率、生命周期事件等。Kubernetes自1.14版本开始默认使用Prometheus，之前版本使用Heapster。

（4）Ingress Controller:Service是一种工作于传统层的负载均衡器，而Ingress 是在应用层实现的HTTP (s）负载均衡机制。不过，Ingress资源自身并不能进行“流量穿透”，它仅是一组路由规则的集合，这些规则需要通过Ingress控制器（ Ingress Controller)发挥作用。目前，此类的可用项目有Nginx、Traefik、Envoy及HAProxy等。

## （三）Kubernetes集群网络

Kubernetes的网络中主要存在四种类型的通信：同－Pod内的容器间通信、各Pod彼此之间的通信、Pod与Service间的通信以及集群外部的流量同Service之间的通信。Kubernetes为Pod和Service资源对象分别使用了各自的专用网络，Pod网络由Kubernetes的网络插件配置实现，而Service的网络则由Kubernetes 集群予以指定。为了提供更灵活的解决方式，Kubernetes的网络模型需要借助于外部插件实现，它要求任何实现机制都必须满足以下需求。

1、所有Pod间均可不经NAT 机制而直接通信。

2、所有节点均可不经NAT机制而直接与所有容器通信。

3、容器自己使用IP也是其他容器或节点直接看到的地址。换句话讲，所有Pod对象都位于同一平面网络中，而且可以使用Pod自身的地址直接通信。



现今大多都采用flannel提供网络，calico提供网络策略控制。

# 二、Kubernetes安装.

 

Kubernetes部署方式可采用kubeadm、ansible、二进制包等的方式安装，以下采用二进制包的方式安装Kubernetes。

## （一）环境初始化

1、安装说明

| 主机名                                   | IP地址      | 系统版本  | 安装服务                                                     |
| ---------------------------------------- | ----------- | --------- | ------------------------------------------------------------ |
| master01.zsjshao.com、etcd01.zsjshao.com | 172.16.0.6  | CentOS7.6 | etcd、kube-apiserver、kube-scheduler、kube-controller-manager |
| master02.zsjshao.com、etcd03.zsjshao.com | 172.16.0.7  | CentOS7.6 | etcd、kube-apiserver、kube-scheduler、kube-controller-manager |
| master03.zsjshao.com、etcd03.zsjshao.com | 172.16.0.8  | CentOS7.6 | etcd、kube-apiserver、kube-scheduler、kube-controller-manager |
| node01.zsjshao.com、store01.zsjshao.com  | 172.16.0.9  | CentOS7.6 | kubelet、kube-proxy、docker、flannel、calico、ceph           |
| node02.zsjshao.com、store01.zsjshao.com  | 172.16.0.10 | CentOS7.6 | kubelet、kube-proxy、docker、flannel、calico、ceph           |
| node03.zsjshao.com、store01.zsjshao.com  | 172.16.0.11 | CentOS7.6 | kubelet、kube-proxy、docker、flannel、calico、ceph           |

 2、设置主机名

```
[root@master ~]# hostnamectl set-hostname master01.zsjshao.com
```

3、域名解析

```
[root@master ~]# vim /etc/hosts

...
172.16.0.6 master01.zsjshao.com etcd01.zsjshao.com master01 etcd01
172.16.0.7 master02.zsjshao.com etcd02.zsjshao.com master02 etcd02
172.16.0.8 master03.zsjshao.com etcd03.zsjshao.com master03 etcd03
172.16.0.9 node01.zsjshao.com store01.zsjshao.com node01 store01
172.16.0.10 node02.zsjshao.com store02.zsjshao.com node02 store02
172.16.0.11 node03.zsjshao.com store03.zsjshao.com node03 store03
```

4、免密认证

```
[root@master01 ~]# ssh-keygen 
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:NXPIlgxg4ZWKimiF4GT0kaQmCAhaOli7WGDfzsH2AUU root@master01.zsjshao.com
The key's randomart image is:
+---[RSA 2048]----+
|Bo+o..*Eo.    |
|OB+o+o...+ o   |
|%o+o *.o X .  |
|o* o= + .o +   |
|o.o. o .S    |
|o..       |
|.        |
|         |
|         |
+----[SHA256]-----+
[root@master01 ~]# ssh-copy-id master01
[root@master01 ~]# ssh-copy-id master02
[root@master01 ~]# ssh-copy-id master03
[root@master01 ~]# ssh-copy-id node01
[root@master01 ~]# ssh-copy-id node02
[root@master01 ~]# ssh-copy-id node03
```

5、禁用firewalld服务

```
[root@master01 ~]# systemctl stop firewalld
[root@master01 ~]# systemctl disable firewalld
```

6、禁用SELinux

```
[root@master01 ~]# sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/selinux/config
[root@master01 ~]# setenforce 0
```

注：各个节点均需要操作

 

## （二）Master节点安装

1、3台Master节点安装配置etcd

（1）生成etcd密钥

i、安装git

```
[root@master01 ~]# yum install git -y
```

ii、克隆源码树

```
[root@master01 ~]# git clone https://github.com/iKubernetes/k8s-certs-generator.git
```

iii、执行脚本，生成密钥

```
[root@master01 ~]# cd k8s-certs-generator/
[root@master01 k8s-certs-generator]# bash gencerts.sh etcd
Enter Domain Name [ilinux.io]: zsjshao.com
```

iiii、分发密钥

```
[root@master01 k8s-certs-generator]# cd etcd
[root@master01 etcd]# cp pki/ /etc/etcd/ -a
[root@master01 etcd]# scp -rp pki/ etcd02:/etc/etcd/
[root@master01 etcd]# scp -rp pki/ etcd03:/etc/etcd/
```

（2）安装etcd软件包

```
[root@master01 ~]# yum install etcd -y
[root@master02 ~]# yum install etcd -y
[root@master03 ~]# yum install etcd -y
```

（3）编辑etcd.conf配置文件

master01节点

```
[root@master01 ~]# cp /etc/etcd/etcd.conf /etc/etcd/etcd.conf.bak
[root@master01 ~]# cat > /etc/etcd/etcd.conf <<EOF
ETCD_DATA_DIR="/var/lib/etcd/k8s.etcd"
ETCD_LISTEN_PEER_URLS="https://172.16.0.6:2380"
ETCD_LISTEN_CLIENT_URLS="https://172.16.0.6:2379"
ETCD_NAME="etcd01.zsjshao.com"
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://etcd01.zsjshao.com:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://etcd01.zsjshao.com:2379"
ETCD_INITIAL_CLUSTER="etcd01.zsjshao.com=https://etcd01.zsjshao.com:2380,etcd02.zsjshao.com=https://etcd02.zsjshao.com:2380,etcd03.zsjshao.com=https://etcd03.zsjshao.com:2380"
ETCD_CERT_FILE="/etc/etcd/pki/server.crt"
ETCD_KEY_FILE="/etc/etcd/pki/server.key"
ETCD_CLIENT_CERT_AUTH="true"
ETCD_TRUSTED_CA_FILE="/etc/etcd/pki/ca.crt"
ETCD_AUTO_TLS="false"
ETCD_PEER_CERT_FILE="/etc/etcd/pki/peer.crt"
ETCD_PEER_KEY_FILE="/etc/etcd/pki/peer.key"
ETCD_PEER_CLIENT_CERT_AUTH="true"
ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/pki/ca.crt"
ETCD_PEER_AUTO_TLS="false"
EOF
```

master02节点

```
[root@master02 ~]# cp /etc/etcd/etcd.conf /etc/etcd/etcd.conf.bak
[root@master02 ~]# cat > /etc/etcd/etcd.conf <<EOF
ETCD_DATA_DIR="/var/lib/etcd/k8s.etcd"
ETCD_LISTEN_PEER_URLS="https://172.16.0.7:2380"
ETCD_LISTEN_CLIENT_URLS="https://172.16.0.7:2379"
ETCD_NAME="etcd02.zsjshao.com"
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://etcd02.zsjshao.com:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://etcd02.zsjshao.com:2379"
ETCD_INITIAL_CLUSTER="etcd01.zsjshao.com=https://etcd01.zsjshao.com:2380,etcd02.zsjshao.com=https://etcd02.zsjshao.com:2380,etcd03.zsjshao.com=https://etcd03.zsjshao.com:2380"
ETCD_CERT_FILE="/etc/etcd/pki/server.crt"
ETCD_KEY_FILE="/etc/etcd/pki/server.key"
ETCD_CLIENT_CERT_AUTH="true"
ETCD_TRUSTED_CA_FILE="/etc/etcd/pki/ca.crt"
ETCD_AUTO_TLS="false"
ETCD_PEER_CERT_FILE="/etc/etcd/pki/peer.crt"
ETCD_PEER_KEY_FILE="/etc/etcd/pki/peer.key"
ETCD_PEER_CLIENT_CERT_AUTH="true"
ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/pki/ca.crt"
ETCD_PEER_AUTO_TLS="false"
EOF
```

master03节点

```
[root@master03 ~]# cp /etc/etcd/etcd.conf /etc/etcd/etcd.conf.bak
[root@master03 ~]# cat > /etc/etcd/etcd.conf <<EOF
ETCD_DATA_DIR="/var/lib/etcd/k8s.etcd"
ETCD_LISTEN_PEER_URLS="https://172.16.0.8:2380"
ETCD_LISTEN_CLIENT_URLS="https://172.16.0.8:2379"
ETCD_NAME="etcd03.zsjshao.com"
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://etcd03.zsjshao.com:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://etcd03.zsjshao.com:2379"
ETCD_INITIAL_CLUSTER="etcd01.zsjshao.com=https://etcd01.zsjshao.com:2380,etcd02.zsjshao.com=https://etcd02.zsjshao.com:2380,etcd03.zsjshao.com=https://etcd03.zsjshao.com:2380"
ETCD_CERT_FILE="/etc/etcd/pki/server.crt"
ETCD_KEY_FILE="/etc/etcd/pki/server.key"
ETCD_CLIENT_CERT_AUTH="true"
ETCD_TRUSTED_CA_FILE="/etc/etcd/pki/ca.crt"
ETCD_AUTO_TLS="false"
ETCD_PEER_CERT_FILE="/etc/etcd/pki/peer.crt"
ETCD_PEER_KEY_FILE="/etc/etcd/pki/peer.key"
ETCD_PEER_CLIENT_CERT_AUTH="true"
ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/pki/ca.crt"
ETCD_PEER_AUTO_TLS="false"
EOF
```

（4）启动etcd服务并设置为开机自动启动

```
[root@master01 ~]# systemctl start etcd && systemctl enable etcd
[root@master02 ~]# systemctl start etcd && systemctl enable etcd
[root@master03 ~]# systemctl start etcd && systemctl enable etcd
```

（5）查看集群状态

```
[root@master01 ~]# etcdctl --endpoints https://etcd01.zsjshao.com:2379 --ca-file /etc/etcd/pki/ca.crt --cert-file /etc/etcd/pki/client.crt --key-file /etc/etcd/pki/client.key cluster-health

member 3b1c867a7e73c7bb is healthy: got healthy result from https://etcd02.zsjshao.com:2379
member 8f7b76ed4db82e0a is healthy: got healthy result from https://etcd03.zsjshao.com:2379
member b12c47b49ab76d6e is healthy: got healthy result from https://etcd01.zsjshao.com:2379
cluster is healthy
```

2、密钥生成

（1）生成k8s master组件所需的密钥

```
[root@master01 k8s-certs-generator]# bash gencerts.sh k8s
Enter Domain Name [ilinux.io]: zsjshao.com
Enter Kubernetes Cluster Name [kubernetes]: 
Enter the IP Address in default namespace 
 of the Kubernetes API Server[10.96.0.1]: 
Enter Master servers name[master01 master02 master03]:
```

（2）密钥分发

```
[root@master01 k8s-certs-generator]# mkdir /etc/kubernetes/
[root@master01 k8s-certs-generator]# cp -pr kubernetes/master01/* /etc/kubernetes/
[root@master01 k8s-certs-generator]# ssh master02 'mkdir /etc/kubernetes'
[root@master01 k8s-certs-generator]# scp -rp kubernetes/master02/* master02:/etc/kubernetes/
[root@master01 k8s-certs-generator]# ssh master03 'mkdir /etc/kubernetes'
[root@master01 k8s-certs-generator]# scp -rp kubernetes/master03/* master03:/etc/kubernetes/
[root@master01 k8s-certs-generator]# scp kubernetes/master01/pki/sa.* master02:/etc/kubernetes/pki/
[root@master01 k8s-certs-generator]# scp kubernetes/master01/pki/sa.* master03:/etc/kubernetes/pki/
```

3、获取master二进制安装包

​                 图7 二进制安装包                

​                ![img](file:///C:/Users/zengs/AppData/Local/Temp/msohtmlclip1/01/clip_image016.jpg)    
链接：https://dl.k8s.io/v1.15.2/kubernetes-server-linux-amd64.tar.gz

4、解压二进制包

```
[root@master01 ~]# tar xf kubernetes-server-linux-amd64.tar.gz -C /usr/local/
[root@master01 ~]# scp -pr /usr/local/kubernetes master02:/usr/local/
[root@master01 ~]# scp -pr /usr/local/kubernetes master03:/usr/local/
```

5、克隆k8s相关配置文件

```
[root@master01 ~]# git clone https://github.com/iKubernetes/k8s-bin-inst.git
[root@master01 ~]# scp k8s-bin-inst/master/unit-files/* master01:/usr/lib/systemd/system/
[root@master02 ~]# scp k8s-bin-inst/master/unit-files/* master02:/usr/lib/systemd/system/
[root@master03 ~]# scp k8s-bin-inst/master/unit-files/* master03:/usr/lib/systemd/system/
```

6、配置apiserver

 master01节点

（1）生成apiserver配置文件

```
[root@master01 ~]# cat > /etc/kubernetes/apiserver <<EOF
KUBE_API_ADDRESS="--advertise-address=0.0.0.0"
KUBE_API_PORT="--secure-port=6443 --insecure-port=0"
KUBE_ETCD_SERVERS="--etcd-servers=https://etcd01.zsjshao.com:2379,https://etcd02.zsjshao.com:2379,https://etcd03.zsjshao.com:2379"
KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.96.0.0/12"
KUBE_ADMISSION_CONTROL="--enable-admission-plugins=NodeRestriction"
KUBE_API_ARGS="--authorization-mode=Node,RBAC \\
  --client-ca-file=/etc/kubernetes/pki/ca.crt \\
  --enable-bootstrap-token-auth=true \\
  --etcd-cafile=/etc/etcd/pki/ca.crt \\
  --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt \\
  --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key \\
  --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt \\
  --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key \\
  --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\
  --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt \\
  --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key \\
  --requestheader-allowed-names=front-proxy-client \\
  --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt \\
  --requestheader-extra-headers-prefix=X-Remote-Extra- \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User\\
  --service-account-key-file=/etc/kubernetes/pki/sa.pub \\
  --tls-cert-file=/etc/kubernetes/pki/apiserver.crt \\
  --tls-private-key-file=/etc/kubernetes/pki/apiserver.key \\
  --token-auth-file=/etc/kubernetes/token.csv"
EOF
```

（2）创建kube用户

```
[root@master01 ~]# useradd -r kube
```

（3）创建状态存储目录，且目录属主属组为kube

```
[root@master01 ~]# mkdir /var/run/kubernetes
[root@master01 ~]# chown kube.kube /var/run/kubernetes
```

（4）重载systemd守护进程

```
[root@master01 ~]# systemctl daemon-reload
```

（5）启动kube-apiserver服务，设置为开机启动

```
[root@master01 ~]# systemctl start kube-apiserver
[root@master01 ~]# systemctl enable kube-apiserver
```

（6）配置kubectl

```
[root@master01 ~]# ln -sv /usr/local/kubernetes/server/bin/kubectl /usr/bin/
[root@master01 ~]# mkdir ~/.kube
[root@master01 ~]# cp /etc/kubernetes/auth/admin.conf ~/.kube/config
```

（7）boostrapper授权

```
[root@master01 ~]# kubectl create clusterrolebinding system:bootstrapper --user=system:bootstrapper --clusterrole=system:node-bootstrapper
```



master02节点

（1）拷贝master01上apiserver配置文件

```
[root@master01 ~]# scp /etc/kubernetes/config master02:/etc/kubernetes/
[root@master01 ~]# scp /etc/kubernetes/apiserver master02:/etc/kubernetes/
```

（2）重载systemd守护进程

```
[root@master02 ~]# systemctl daemon-reload
```

（3）创建kube用户，

```
[root@master02 ~]# useradd -r kube
```

（4）创建状态存储目录，且目录属主属组为kube

```
[root@master02 ~]# mkdir /var/run/kubernetes
[root@master02 ~]# chown kube.kube /var/run/kubernetes
```

（5）启动kube-apiserver服务，设置为开机启动

```
[root@master02 ~]# systemctl start kube-apiserver
[root@master02 ~]# systemctl enable kube-apiserver
```

（6）配置kubectl

```
[root@master02 ~]# ln -sv /usr/local/kubernetes/server/bin/kubectl /usr/bin/
[root@master02 ~]# mkdir ~/.kube
[root@master02 ~]# cp /etc/kubernetes/auth/admin.conf ~/.kube/config
```

master03节点

（1）拷贝master01上apiserver配置文件

```
[root@master01 ~]# scp /etc/kubernetes/config master03:/etc/kubernetes/
[root@master01 ~]# scp /etc/kubernetes/apiserver master03:/etc/kubernetes/
```

（2）重载systemd守护进程

```
[root@master03 ~]# systemctl daemon-reload
```

（3）创建kube用户，

```
[root@master03 ~]# useradd -r kube
```

（4）创建状态存储目录，且目录属主属组为kube

```
[root@master03 ~]# mkdir /var/run/kubernetes
[root@master03 ~]# chown kube.kube /var/run/kubernetes
```

（5）启动kube-apiserver服务，设置为开机启动

```
[root@master03 ~]# systemctl start kube-apiserver
[root@master03 ~]# systemctl enable kube-apiserver
```

（6）配置kubectl

```
[root@master03 ~]# ln -sv /usr/local/kubernetes/server/bin/kubectl /usr/bin/
[root@master03 ~]# mkdir ~/.kube
[root@master03 ~]# cp /etc/kubernetes/auth/admin.conf ~/.kube/config
```

7、配置controller-manager

 master01节点

（1）生成controller-manager配置文件

```
[root@master01 ~]# cat > /etc/kubernetes/controller-manager <<EOF
KUBE_CONTROLLER_MANAGER_ARGS="--bind-address=127.0.0.1 \\
  --allocate-node-cidrs=true \\
  --authentication-kubeconfig=/etc/kubernetes/auth/controller-manager.conf \\
  --authorization-kubeconfig=/etc/kubernetes/auth/controller-manager.conf \\
  --client-ca-file=/etc/kubernetes/pki/ca.crt \\
  --cluster-cidr=10.244.0.0/16 \\
  --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt \\
  --cluster-signing-key-file=/etc/kubernetes/pki/ca.key \\
  --controllers=*,bootstrapsigner,tokencleaner \\
  --kubeconfig=/etc/kubernetes/auth/controller-manager.conf \\
  --leader-elect=true \\
  --node-cidr-mask-size=24 \\
  --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt \\
  --root-ca-file=/etc/kubernetes/pki/ca.crt \\
  --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\
  --use-service-account-credentials=true"
EOF
```

（2）启动kube-apiserver服务，设置为开机启动

```
[root@master01 ~]# systemctl start kube-controller-manager
[root@master01 ~]# systemctl enable kube-controller-manager
```

master02节点

（1）拷贝master01上controller-manager配置文件

```
[root@master01 ~]# scp /etc/kubernetes/controller-manager master02:/etc/kubernetes/
```

（2）启动kube-apiserver服务，设置为开机启动

```
[root@master02 ~]# systemctl start kube-controller-manager
[root@master02 ~]# systemctl enable kube-controller-manager
```

master03节点

（1）拷贝master01上controller-manager配置文件

```
[root@master01 ~]# scp /etc/kubernetes/controller-manager master03:/etc/kubernetes/
```

（2）启动kube-apiserver服务，设置为开机启动

```
[root@master03 ~]# systemctl start kube-controller-manager
[root@master03 ~]# systemctl enable kube-controller-manager
```

7、配置scheduler

master01节点

（1）生成scheduler配置文件

```
[root@master01 ~]# cat > /etc/kubernetes/scheduler <<EOF
KUBE_SCHEDULER_ARGS="--address=127.0.0.1 \\
  --kubeconfig=/etc/kubernetes/auth/scheduler.conf \\
  --leader-elect=true"
EOF
```

（2）启动kube-scheduler服务，设置为开机启动

```
[root@master01 ~]# systemctl start kube-scheduler
[root@master01 ~]# systemctl enable kube-scheduler
```

**master02节点**

（1）生成scheduler配置文件

```
[root@master02 ~]# cat > /etc/kubernetes/scheduler <<EOF
KUBE_SCHEDULER_ARGS="--address=127.0.0.1 \\
  --kubeconfig=/etc/kubernetes/auth/scheduler.conf \\
  --leader-elect=true"
EOF
```

（2）启动kube-scheduler服务，设置为开机启动

```
[root@master02 ~]# systemctl start kube-scheduler
[root@master02 ~]# systemctl enable kube-scheduler
```

master03节点

（1）生成scheduler配置文件

```
[root@master03 ~]# cat > /etc/kubernetes/scheduler <<EOF
KUBE_SCHEDULER_ARGS="--address=127.0.0.1 \\
  --kubeconfig=/etc/kubernetes/auth/scheduler.conf \\
  --leader-elect=true"
EOF
```

（2）启动kube-scheduler服务，设置为开机启动

```
[root@master03 ~]# systemctl start kube-scheduler
[root@master03 ~]# systemctl enable kube-scheduler
```

## （三）Node节点安装

 1、安装配置docker

（1）添加yum源

```
[root@node01 ~]# cd /etc/yum.repos.d/
[root@node01 yum.repos.d]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
```

（2）安装docker-ce软件包

```
[root@node01 ~]# yum install docker-ce -y
```

（3）修改docker unit配置文件

```
[root@node01 ~]# vim /usr/lib/systemd/system/docker.service
[Service]
...
ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT
...
```

（4）重载systemd

```
[root@node01 ~]# systemctl daemon-reload
```

（5）装载bridge-nf-call

```
[root@node01 ~]# vim /etc/sysctl.conf
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
```

（6）启用bridge-nf-call

```
[root@node01 ~]# sysctl -p
```

（7）启动docker，并设为开机启动

```
[root@node01 ~]# systemctl start docker
[root@node01 ~]# systemctl enable docker
```

2、获取node二进制安装包

链接：https://dl.k8s.io/v1.15.2/kubernetes-node-linux-amd64.tar.gz

​                ![img](file:///C:/Users/zengs/AppData/Local/Temp/msohtmlclip1/01/clip_image018.jpg)    
 

​         图8 二进制安装包    



3、解压二进制包

```
[root@node01 ~]# tar xf kubernetes-node-linux-amd64.tar.gz -C /usr/local/
```

 4、提供CNI插件

链接：https://github.com/containernetworking/plugins/releases/download/v0.8.2/cni-plugins-linux-amd64-v0.8.2.tgz

（1）解压软件包

```
[root@node01 ~]# mkdir -p /opt/cni/bin
[root@node01 ~]# tar xf cni-plugins-linux-amd64-v0.8.2.tgz -C /opt/cni/bin/
```

5、配置kubelet

（1）在master01上拷贝配置文件

```
[root@master01 ~]# cd /root/k8s-bin-inst/nodes/var/lib/
[root@master01 lib]# scp -pr * node01:/var/lib/

[root@master01 ~]# cd /root/k8s-bin-inst/nodes/etc/
[root@master01 etc]# scp -pr kubernetes node01:/etc/

[root@master01 ~]# cd /root/k8s-bin-inst/nodes/unit-files/
[root@master01 unit-files]# scp * node01:/usr/lib/systemd/system/
```

（2）在master01上拷贝证书

```
[root@master01 ~]# cd /root/k8s-certs-generator/kubernetes/kubelet
[root@master01 kubelet]# scp -pr * node01:/etc/kubernetes/
```

（3）启动kubelet服务，并设置为开机启动

```
[root@node01 ~]# systemctl daemon-reload
[root@node01 ~]# systemctl start kubelet
[root@node01 ~]# systemctl enable kubelet
```

（4）在master节点上签署证书

```
[root@master01 kubernetes]# kubectl get csr
[root@master01 kubernetes]# kubectl certificate approve csr-g49m5
```

其他node参照node01的配置

 

6、配置kube-proxy

（1）装载ipvs模块

```
[root@node01 ~]# vim /etc/sysconfig/modules/ipvs.modules
#!/bin/bash
ipvs_mods_dir="/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs"
for mod in $(ls $ipvs_mods_dir | grep -o "^[^.]*"); do
  /sbin/modinfo -F filename $mod &> /dev/null
  if [ $? -eq 0 ]; then
    /sbin/modprobe $mod
  fi
done
 
[root@node01 ~]# chmod +x /etc/sysconfig/modules/ipvs.modules
```

（2）启动kube-proxy，并设置为开机启动

```
[root@node01 ~]# systemctl start kube-proxy
[root@node01 ~]# systemctl enable kube-proxy
```

其他node参照node01的配置

##  （四）flannel+calico+CoreDNS节点安装

1、部署flannel

```
[root@master01 ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

2、部署calico

```
[root@master01 ~]# kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/canal.yaml
```

 3、部署CoreDNS

[root@master01 ~]# mkdir coredns && cd coredns

```
[root@master01 ~]# mkdir coredns && cd coredns
[root@master01 coredns]# wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed
[root@master01 coredns]# wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.sh
[root@master01 coredns]# bash deploy.sh -i 10.96.0.10 -r "10.96.0.0/12" -s -t coredns.yaml.sed | kubectl apply -f -
```

4、获取节点信息

```
[root@master01 ~]# kubectl get nodes
NAME         STATUS  ROLES  AGE  VERSION
node01.zsjshao.com  Ready  <none>  35m  v1.15.2
node02.zsjshao.com  Ready  <none>  35m  v1.15.2
node03.zsjshao.com  Ready  <none>  53s  v1.15.2
```

## （五）ceph存储安装配置

1、环境初始化

（1）3个存储节点添加ceph N版软件仓库

```
[root@node01 ~]# rpm -ivh https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm
[root@node02 ~]# rpm -ivh https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm
[root@node03 ~]# rpm -ivh https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm
```

（2）3个存储节点添加cephadm管理用户并设置密码

```
[root@node01 ~]# useradd cephadm && echo "shaoji" | passwd --stdin cephadm
[root@node02 ~]# useradd cephadm && echo "shaoji" | passwd --stdin cephadm
[root@node03 ~]# useradd cephadm && echo "shaoji" | passwd --stdin cephadm
```

 （3）3个存储节点设置cephadm用户无密码执行sudo命令

```
[root@node01 ~]# echo "cephadm ALL = (root) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/cephadm
[root@node02 ~]# echo "cephadm ALL = (root) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/cephadm
[root@node03 ~]# echo "cephadm ALL = (root) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/cephadm
```

（4）切换到普通用户cephadm，实现与存储节点免密认证

```
[cephadm@node01 ~]$ ssh-keygen
[cephadm@node01 ~]$ ssh-copy-id node01
[cephadm@node01 ~]$ ssh-copy-id node02
[cephadm@node01 ~]$ ssh-copy-id node03
```

2、安装ceph存储

（1）安装ceph-deploy部署工具

```
[root@node01 ~]# yum install ceph-deploy python-setuptools python2-subprocess32 -y
```

（2）创建ceph-cluster目录，初始化ceph集群配置，并设置第一个mon节点

```
[cephadm@node01 ~]$ mkdir ceph-cluster && cd ceph-cluster
[cephadm@node01 ceph-cluster]$ ceph-deploy new --cluster-network 192.168.0.0/24 --public-network 172.16.0.0/24 node01.zsjshao.com node02.zsjshao.com node03.zsjshao.com
```

（3）安装ceph集群

```
[cephadm@node01 ceph-cluster]$ ceph-deploy install --no-adjust-repos node01 node02 node03
```

（4）初始化mon节点，并收集所有密钥

```
[cephadm@node01 ceph-cluster]$ ceph-deploy mon create-initial
```

（5）设置管理员节点

```
[cephadm@node01 ceph-cluster]$ ceph-deploy admin node01 node02 node03
```

 （6）添加mgr节点

```
[cephadm@node01 ceph-cluster]$ ceph-deploy mgr create node01
[cephadm@node01 ceph-cluster]$ ceph-deploy mgr create node02
```

（7）列出可用磁盘

```
[root@node01 ceph-cluster]# ceph-deploy disk list node01 node02 node03
```

 （8）擦除磁盘上的数据

```
[root@node01 ceph-cluster]# ceph-deploy disk zap node01 /dev/sdb
[root@node01 ceph-cluster]# ceph-deploy disk zap node02 /dev/sdb 
[root@node01 ceph-cluster]# ceph-deploy disk zap node03 /dev/sdb 
```

（9）添加OSD

```
[cephadm@node01 ceph-cluster]$ ceph-deploy osd create --data /dev/sdb node01
[cephadm@node01 ceph-cluster]$ ceph-deploy osd create --data /dev/sdb node02
[cephadm@node01 ceph-cluster]$ ceph-deploy osd create --data /dev/sdb node03
```

（10）查看集群状态

```
[cephadm@node01 ceph-cluster]$ ceph -s
 cluster:
  id:   4e8a2de9-e790-4a14-a6a0-1923f80189b4
  health: HEALTH_OK

 services:
  mon: 3 daemons, quorum node01,node02,node03 (age 61m)
  mgr: node01(active, since 61m), standbys: node02
  osd: 3 osds: 3 up (since 19m), 3 in (since 19m)

 data:
  pools:  0 pools, 0 pgs
  objects: 0 objects, 0 B
  usage:  3.0 GiB used, 594 GiB / 597 GiB avail
  pgs:
```

3、rbd volume

（1）创建kube存储池

```
[root@node01 ~]# ceph osd pool create kube 64 64
pool 'kube' created
```

（2）创建kube账号，此账号拥有访问kube存储池的相关权限

```
[root@node01 ~]# ceph auth get-or-create client.kube mon 'allow r' osd 'allow * pool=kube'
[client.kube]
 key = AQDQfGNdfgbLAhAAr6Zusi+y5XLcpDz5hQr74g==
```

（3）获取client.admin和client.kube的用户密钥

```
[root@node01 ~]# ceph auth print-key client.admin | base64
QVFCRzJWWmQ1alhHRVJBQWU5bUpXbUdiMmF2ZFlDOGFmcjh6Mnc9PQ==
[root@node01 ~]# ceph auth print-key client.kube | base64
QVFEUWZHTmRmZ2JMQWhBQXI2WnVzaSt5NVhMY3BEejVoUXI3NGc9PQ==
```

（4）所有节点安装ceph-common软件包

```
[root@master01 ~]# yum install ceph-common -y
```

（5）生成存储类rbd-kube的yaml配置文件

```
[root@master01 ~]# vim rbd-kube.yaml 
apiVersion: v1
kind: Secret
metadata:
 name: ceph-admin-secret
 namespace: kube-system
type: "kubernetes.io/rbd"
data:
 key: QVFCRzJWWmQ1alhHRVJBQWU5bUpXbUdiMmF2ZFlDOGFmcjh6Mnc9PQ==
---
apiVersion: v1
kind: Secret
metadata:
 name: ceph-kube-secret
type: "kubernetes.io/rbd"
data:
 key: QVFEUWZHTmRmZ2JMQWhBQXI2WnVzaSt5NVhMY3BEejVoUXI3NGc9PQ==
---
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
 name: rbd-kube
 annotations:
  storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
 monitors: node01:6789,node02:6789,node03:6789
 adminId: admin
 adminSecretName: ceph-admin-secret
 adminSecretNamespace: kube-system
 pool: kube
 userId: kube
 userSecretName: ceph-kube-secret
reclaimPolicy: Retain
[root@master01 ~]#
```

 （6）创建存储类rbd-kube

```
[root@master01 ~]# kubectl apply -f rbd-kube.yaml 
secret/ceph-admin-secret created
secret/ceph-kube-secret created
storageclass.storage.k8s.io/rbd-kube created
[root@master01 ~]#
```

（7）查看存储类

```
[root@master01 ~]# kubectl get sc
NAME         PROVISIONER     AGE
rbd-kube (default)  kubernetes.io/rbd  4m20s
```

# 三、应用部署

WordPress是一个以PHP和MySQL为平台的自由开源的博客软件和内容管理系统。WordPress具有插件架构和模板系统。截至2018年4月，排名前1000万的网站中超过30.6%使用WordPress。WordPress是最受欢迎的网站内容管理系统。WordPress是当前因特网上最流行的博客系统。WordPress在最著名的网络发布阶段中脱颖而出。如今，它被使用在超过7000万个站点上。

## （一）安装mariadb

1、编辑mariadb.yaml文件，提供mariadb服务

```
[root@master01 ~]# vim mariadb.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-mariadb-wordpress
spec:
  storageClassName: rbd-kube
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: Service
metadata:
  name: mariadb-wordpress
  labels:
    app: mariadb-wordpress
spec:
  clusterIP: None
  ports:
    - port: 3306
  targetPort: 3306
  selector:
    app: mariadb-wordpress
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
 name: mariadb-wordpress
spec:
 replicas: 1
 selector:
  matchLabels:
   app: mariadb-wordpress
 serviceName: "mariadb-wordpress"
 template:
  metadata:
   labels:
     app: mariadb-wordpress
  spec:
   containers:
   - name: mariadb-wordpress
     image: mariadb
     env: 
   - name: MYSQL_ROOT_PASSWORD
     value: root 
     volumeMounts: 
   - name: data
     mountPath: /var/lib/mysql
   volumes:
   - name: data
     persistentVolumeClaim:
     claimName: rbd-mariadb-wordpress
```

2、应用mariadb.yaml配置文件，提供mariadb服务

```
[root@master01 ~]# kubectl apply -f mariadb.yaml 
persistentvolumeclaim/rbd-mariadb-wordpress created
service/mariadb-wordpress created
statefulset.apps/mariadb-wordpress created
```

3、查看mariadb的pod状态

```
[root@master01 ~]# kubectl get pods
NAME         READY  STATUS  RESTARTS  AGE
mariadb-wordpress-0  1/1   Running  0     2m59s
```

4、连接mariadb pod，创建wordpress所需数据库，账号密码

```
[root@master01 ~]# kubectl exec -it mariadb-wordpress-0 bash
root@mariadb-wordpress-0:/# mysql -uroot -proot
Welcome to the MariaDB monitor. Commands end with ; or \g.
Your MariaDB connection id is 8
Server version: 10.4.7-MariaDB-1:10.4.7+maria~bionic mariadb.org binary distribution
 
Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.
 
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
 
MariaDB [(none)]> create database wpdb;
Query OK, 1 row affected (0.001 sec)
 
MariaDB [(none)]> grant all on wpdb.* to 'wpuser'@'%' identified by 'wppass';
Query OK, 0 rows affected (0.005 sec)
 
MariaDB [(none)]> quit
Bye
root@mariadb-wordpress-0:/#
```

## （二）安装wordpress

1、编辑wordpress.yaml文件，提供wordpress站点服务

```
[root@master01 ~]# vim wordpress.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: rbd-data-wordpress
spec:
 storageClassName: rbd-kube
 accessModes:
  - ReadWriteOnce
 resources:
  requests:
   storage: 5Gi
---
apiVersion: v1
kind: Service
metadata:
 name: wordpress
 labels:
  app: wordpress
spec:
 type: NodePort
 ports:
 - port: 80 
  nodePort: 30080
  targetPort: 80
 selector:
  app: wordpress
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: wordpress
spec:
 replicas: 1
 selector:
  matchLabels:
   app: wordpress
 template:
  metadata:
   labels:
    app: wordpress
  spec:
   containers:
   - name: wordpress
     image: wordpress
   env: 
    - name: WORDPRESS_DB_HOST
      value: mariadb-wordpress.default.svc.cluster.local
    - name: WORDPRESS_DB_USER
      value: wpuser
    - name: WORDPRESS_DB_PASSWORD
      value: wppass
    - name: WORDPRESS_DB_NAME
      value: wpdb
    volumeMounts: 
    - name: data
      mountPath: /var/www/html
   volumes:
   - name: data
     persistentVolumeClaim:
     claimName: rbd-data-wordpress
```

2、应用wordpress.yaml配置文件，提供wordpress站点服务

```
[root@master01 ~]# kubectl apply -f wordpress.yaml 
persistentvolumeclaim/rbd-data-wordpress created
service/wordpress created
deployment.apps/wordpress created
```

3、查看wordpress的pod状态

```
[root@master01 ~]# kubectl get pods
NAME             READY  STATUS  RESTARTS  AGE
mariadb-wordpress-0     1/1   Running  0     58m
wordpress-59784b7cf7-mpwmf  1/1   Running  0     52s
```

## （三）配置wordpress站点

1、输入node节点地址，端口为30080，选择“简体中文”，点击“继续”

2、添加站点标题、用户名、密码、电子邮件信息，点击安装wordpress

3、安装完成，点击登录

4、输入admin账号密码进行登录

5、查看后台管理员界面

6、查看默认主页

# 四、结论

Kubernetes是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。

通过Kubernetes能带来的以下几点好处：

1）节约成本：Kubernetes集群以运维工作量少而闻名。企业团队不必编写自己的容器化脚本。团队成员也不必在“重新发明轮子”或者放弃共享基础设施的优势之间做出选择。还可以通过使用容器更加有效的降低硬件成本。

2）更短的交付周期： Kubernetes非常适合DevOps。良好的容器管理意味着只要软件运行，部署几乎总是无痛，这意味着更快部署。

3）IT灵活性：在现代企业中，软件可以在任意数量的私有和共享基础架构上运行。拥有容器管理解决方案意味着不必牺牲性能或进行重大调整来移动您的应用程序。您可以在任何业务需要的地方运行软件。这使团队更加灵活。

Kubernetes所擅长的，是按照用户的意愿和整个系统的规则，完全自动化的处理好容器之间的各种关系。其本质，是为用户提供一个具有普遍意义的容器编排工具。可以说，在容器云中，Kubernetes提供了一套基于容器构建分布式系统的基础依赖，其意义等同于Linux在操作系统中的地位。

# 致谢

虽说求学之路并非一帆风顺，但经历过磕磕碰碰之后仍觉收获颇多，也对未来多了一份执着和憧憬。按照惯例，总要说些感谢的话，虽然有些俗套，但确实发自内心。感谢我的导师。从论文的选题到写作直至最后完稿，老师总是在繁忙的工作之余，抽出宝贵的时间给予我论文上的指导和帮助，使我能够顺利完成论文写作。老师平易近人、诲人不倦、宽容大度、对学生认真负责，他为人处世的态度和严谨治学的精神深深影响着我，并将在我以后的人生道路上一直激励我。在此，谨向老师致以深深的敬意，并表示由衷的感谢。

# 文献

 **[1]**.龚正、吴治辉、崔秀龙、闫健勇，Kubernetes权威指南，电子工业出版社，2019年6月第4版

**[2]**. 闫健勇、龚正、吴治辉、刘晓红、崔秀龙、赵玲丽、何通，Kubernetes权威指南企业级容器云实战，电子工业出版社，2018年8月第1版

**[3]**.CloudMan，每天5分钟玩转Kubernetes，清华大学出版社，2018年8月第1版

**[4]**.CloudMan，每天5分钟玩转Docker容器技术，清华大学出版社，2018年8月第1版

**[5]**.马永亮，Kubernetes进阶实战，机械工业出版社，2019年1月第1版

**[6]**.杨保华，Docker技术入门与实战，机械工业出版社，2018年9月第3版

**[7]**.华为Docker实践小组，Docker进阶与实战，机械工业出版社，2018年9月第3版

**[8]**.kubernetes官网，https://kubernetes.io
